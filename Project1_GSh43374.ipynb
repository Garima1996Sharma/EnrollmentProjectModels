{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10167d3",
   "metadata": {},
   "source": [
    "## 1) Prediction on enrollment of courses\n",
    "### Name: Garima Sharma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf109c",
   "metadata": {},
   "source": [
    "## 2) Library import and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dee9ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Ridge, Lasso \n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "492e80e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>number_ratings</th>\n",
       "      <th>enrollment</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>lectures</th>\n",
       "      <th>duration</th>\n",
       "      <th>price2</th>\n",
       "      <th>discount</th>\n",
       "      <th>inst_rating</th>\n",
       "      <th>inst_review</th>\n",
       "      <th>inst_student</th>\n",
       "      <th>inst_course</th>\n",
       "      <th>cat_design</th>\n",
       "      <th>cat_development</th>\n",
       "      <th>cat_hobby</th>\n",
       "      <th>cat_it_software</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>6833.0</td>\n",
       "      <td>21218.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.23</td>\n",
       "      <td>23.99</td>\n",
       "      <td>91.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>19993.0</td>\n",
       "      <td>55621.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.5</td>\n",
       "      <td>2826.0</td>\n",
       "      <td>14269.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>41.0</td>\n",
       "      <td>7.13</td>\n",
       "      <td>19.99</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>11922.0</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.4</td>\n",
       "      <td>991.0</td>\n",
       "      <td>3563.0</td>\n",
       "      <td>20.67</td>\n",
       "      <td>91.0</td>\n",
       "      <td>5.23</td>\n",
       "      <td>28.99</td>\n",
       "      <td>77.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2193.0</td>\n",
       "      <td>7422.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.2</td>\n",
       "      <td>681.0</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>57.0</td>\n",
       "      <td>5.05</td>\n",
       "      <td>16.99</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>25989.0</td>\n",
       "      <td>242683.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.3</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>14048.0</td>\n",
       "      <td>13.33</td>\n",
       "      <td>142.0</td>\n",
       "      <td>10.77</td>\n",
       "      <td>20.99</td>\n",
       "      <td>55.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>25581.0</td>\n",
       "      <td>113480.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_rating  number_ratings  enrollment  last_updated  lectures  duration  \\\n",
       "0         4.6          6833.0     21218.0         20.83      53.0      5.23   \n",
       "1         4.5          2826.0     14269.0         20.83      41.0      7.13   \n",
       "2         4.4           991.0      3563.0         20.67      91.0      5.23   \n",
       "3         4.2           681.0      1932.0         20.83      57.0      5.05   \n",
       "4         4.3          1939.0     14048.0         13.33     142.0     10.77   \n",
       "\n",
       "   price2  discount  inst_rating  inst_review  inst_student  inst_course  \\\n",
       "0   23.99      91.0          4.6      19993.0       55621.0          3.0   \n",
       "1   19.99      88.0          4.5      11922.0       53659.0         10.0   \n",
       "2   28.99      77.0          4.4       2193.0        7422.0          6.0   \n",
       "3   16.99      64.0          4.3      25989.0      242683.0         43.0   \n",
       "4   20.99      55.0          4.5      25581.0      113480.0         12.0   \n",
       "\n",
       "   cat_design  cat_development  cat_hobby  cat_it_software  \n",
       "0           0                1          0                0  \n",
       "1           0                0          0                0  \n",
       "2           0                0          0                0  \n",
       "3           0                0          0                1  \n",
       "4           0                0          0                0  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data\n",
    "df = pd.read_csv('Dataset/Lab02_GSh43374_prepared.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "679b1c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avg_rating         0\n",
       "number_ratings     0\n",
       "enrollment         0\n",
       "last_updated       0\n",
       "lectures           0\n",
       "duration           0\n",
       "price2             0\n",
       "discount           0\n",
       "inst_rating        0\n",
       "inst_review        0\n",
       "inst_student       0\n",
       "inst_course        0\n",
       "cat_design         0\n",
       "cat_development    0\n",
       "cat_hobby          0\n",
       "cat_it_software    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## checking count of null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed3aafd",
   "metadata": {},
   "source": [
    "## 3) Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d7daa2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1455 entries, 0 to 1454\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   avg_rating       1455 non-null   float64\n",
      " 1   number_ratings   1455 non-null   float64\n",
      " 2   enrollment       1455 non-null   float64\n",
      " 3   last_updated     1455 non-null   float64\n",
      " 4   lectures         1455 non-null   float64\n",
      " 5   duration         1455 non-null   float64\n",
      " 6   price2           1455 non-null   float64\n",
      " 7   discount         1455 non-null   float64\n",
      " 8   inst_rating      1455 non-null   float64\n",
      " 9   inst_review      1455 non-null   float64\n",
      " 10  inst_student     1455 non-null   float64\n",
      " 11  inst_course      1455 non-null   float64\n",
      " 12  cat_design       1455 non-null   int64  \n",
      " 13  cat_development  1455 non-null   int64  \n",
      " 14  cat_hobby        1455 non-null   int64  \n",
      " 15  cat_it_software  1455 non-null   int64  \n",
      "dtypes: float64(12), int64(4)\n",
      "memory usage: 182.0 KB\n"
     ]
    }
   ],
   "source": [
    "## summary statistics of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a1b9c1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbIAAAFICAYAAAA1entjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABbfElEQVR4nO2deZwkRbW2n3eGgQGGVUAFRRBRBEWQYZNFcLvqRQFlEXFBUMQV3PXCVUS5gnL9BDdERFCQHRS4CAiy79sw7IuAOooisg7rzPT7/RFRTE5NdVdmZ3Z3Vfd5+pe/rszKOBlVnV2n4sSJ98g2QRAEQdCvTBrrDgRBEARBHcKRBUEQBH1NOLIgCIKgrwlHFgRBEPQ14ciCIAiCviYcWRAEQdDXhCMLgiAIGkHSUZIelHTLIM9L0mGS7pE0U9Lrm7huOLIgCIKgKY4G3j7E8+8A1szbnsBPm7hoOLIgCIKgEWxfAjw8xCnbAr9y4ipgWUkvrnvdReoaCJplzkP31pZaOfZ1X6/dj8m1LcA5izxZq/0Sqt+LJz23to1/DTxd28aLJy1Zq/0RX3957T4c+41/1LbxUM0/yaUe6jOuHGtNWrq2jUepf1+sP29qrfbHzP1z7T5c98ClqmujymfOoiuu8XHSSKrFEbaPqHC5VYC/FvZn5WMPVLCxEOHIgiAIJjID80qfmp1WFcfVTifHW/vLeziyIAiCiYwHRvNqs4CXFvZfAvy9rtGYIyuJpK0kvaGwv5ekD41ln4IgCGozMFB+q88ZwIdy9uImwGO2a4UVIUZkCyBpEXvQSZWtgNnAFQC2Dx+tfgVBEIwUnld/vrCFpONJn5UrSJoFfAOYAs9/Zp4NvBO4B3gK+EgT1+0bRybpt6Qh6VTgUFI+wuq2v5yf3w3YwPZnJP03sCtpUvEh4Hrbhwxi9yKSc9oMOEPSXcB+wKLAv7OdxYG9gHmSPgB8BngzMNv2IdnG1cDWwLLAHrYvlbQEKR11LeB2YDXgU7ava+htCYIgqEeDoUXbu3R53sCnGrtgpm8cGbC77YclLQ5cS3IklwNfzs/vDBwoaTrwXmB90uu7Abi+i+1lbb8RQNJywCa2LemjwJdtf0HS4WTHlc97c5uNRWxvJOmdpG8hbwE+CTxie11JrwFm1HkDgiAIGqdCskev0k9zZJ+VdBNwFWlktjpwr6RNJL0AeBXJsW0O/M7207afAM4sYfvEwuOXAOdKuhn4ErBOyf6dln9fTxp5kftyAoDtW4CZnRpK2lPSdZKuO/JXx5e8XBAEQQN4oPzWo/TFiEzSVqQRzqa2n8qhvKkkB7QTcAdweh5FDWddRXHB0w+B79s+I193/5I2ns2/5zH/fS3Vl2JKaxPryIIgCErTTBLHmNIvI7JlSCG6pyStBWySj58GbAfswvxR1WXAuyRNlTQN+M9hXOtv+fGHC8efAJaqaOsykqNF0trAayu2D4IgGFHsgdJbr9IvjuwcYBFJM4FvkcKL2H4EuA14me1r8rFrSSmeN5Ec3XXAYxWutT9wsqRLSYkiLc4Etpc0Q9IWJW39BFgx9/srpNBilb4EQRCMLPPmlt96lL4ILdp+liQ22em5bTocPsT2/jlr8BLgf4ewvVXb/u+A33U47y5g3cKhSzvZsP0Q8+fIngE+YPsZSWsAFwD1dWmCIAiaYhwke/SFIxsGR+RQ3lTgGNs3jFE/lgAulDSFNF/2CdvPjVFfgiAIFqaHQ4ZlGZeOzPb7249J+jFprViRQ23/cgT78QQwfaTsB0EQ1GYcJHuMS0fWCduNL8IbCZpQrv/ATQfUtjH3tktq2zjngwtFaCsxr76WKEtrSm0bcybV/0ev+1p+v29tOTo2mFKvGgHA7GcXrdX+2sXqf+T81fWrEayuJWrbOF+P1mq/4iLTavehEWJEFgRBEPQ1MSILgiAI+hkPzBnrLtSmJ9PvJV2UpabG6vr/1bZ/xVj1JQiCYEQZXfX7EaEnHVkdJHUdZUpdSw8v4Mhsv2GwE4MgCPqacSBRVcuRSVpN0u2Sfi7pVknnSVq8OKKStIKk+/Pj3ST9VtKZku6T9GlJn5d0o6SrJC1fMP8BSVdIukXSRrn9kpKOknRtbrNtwe7Jks4Ezhukr1tJulDSb4Cb87HfSro+933PfOwgYPG88Pm4fGx2wcZFkk6RdIek41qSWJLemY9dJukwSWfl42/MtmbkPldVBwmCIBg5BuaV33qUJubI1gR2sf0xSSeRlOeH4jUkZfqppJo0X7G9vqT/B3wI+EE+b0nbb5C0JXBUbrcv8Efbu0taFrhG0vn5/E2BdW0/PMS1NwJeY/u+vL+Aor6kU21/VdKnba83iI31SULCfyeJFG8m6TrgZ8CWtu/LNXlafJFUuuXyLJn1TJf3JwiCYPTo4ZFWWZoILd5ne0Z+XFR+H4wLbT9h+18kuaaWOv3NbW2PB7B9CbB0dlxvA74qaQZwEckZrprP/0MXJwZwTcGJwcKK+mt2ad+yMctJeGxG7vNawL0F20VHdjnwfUmfJZWLWUjnpah+f9GTd5foQhAEQUOMA4mqJhzZs4XHLeX3uQXbU4c4f6CwP8CCI8T2hTcmqWO81/Z6eVvV9u35+TKLZJ4/p01R/3XAjR362olOr3dQlXvbBwEfJRXnvCqLHrefc4Tt6banb7VkGV8aBEHQEJHsMSj3AxvkxzsM08bOAJI2Bx6z/RhwLvCZwrzU+jX6OJiiPsCcLCtVljuAl0tardj33Mc1bN9s+2CSgPFCjiwIgmDMCEc2KIcAn8hp6ysM08Yjuf3hwB752LeAKcBMSbfk/eHSUVE/c0S+xnFlDNl+mlQN+hxJlwH/ZL7K/T45YeUm4Gng9zX6HARB0Cj2vNJbryI76jg2gaRptmfn0eKPgbtt/7+qdn65ygdq/0F6RaJqj5oSVYup/vesRRv4rvZvP9v9pK796LbiY2h2eHbx2n1YrQmJqjn1JKp+sFj993JyuXq1Q9KERNWdA0/Uav9cA47h93/9fe034+mLjir9mbP4VrvXf/NHgHG3jmwM+VhOQrmVFLb82dh2JwiCoATjYB3ZuJOokvRa4Ndth5+1vfFIXjePviqPwNqp97090cRoapG1t6xt4xlOq9V+sQa+ZzUhPDyngX/gJSfVEy+ep/pfhKcuWj/rbLnln6rVfsqjS9buw5QG7ou5DdwXU7trLwzJ4wP1R6eN0MPZiGUZd47M9s3AemPdjyAIgr6gh5M4yjLuHFkQBEFQgR4OGZZlws6RSTpa0g758YiLFEvaR2pghjkIgqBJGky/l/R2SXdKukfSVzs8v0yWKLwpSwN+pImXMK4dWRkB4VFkHyAcWRAEvUVDjiyLsf8YeAewNrCLpLXbTvsUcFsWodgK+F9J9VJh6RNHJukDkq7Jwrs/kzRZ0mxJB2bPfpWkF+Zzj5b0fUkXAgdLWi8/P1PS6ZKW63Kt2ZIOzmLC50vaKI/Y7pX07nzOZEnfy+LFMyV9PB/vKCqc5alWBi7M/QqCIOgNmsta3Ai4x/a9tp8DTgC2bb8asFRepjQNeJikBFWLnndkkl5NUsrYLAv5zgN2BZYErsqe/RLgY4VmrwTeYvsLwK9IwsTrkvQcv9HlkksCF9neAHgC+DbwVmB7oLVAaw+S2siGwIak1PvV83Prk0ZfawMvz/0+jCQyvLXtrYfzPgRBEIwIFbQWi7qweduzYGkV4K+F/Vn5WJEfAa8mfR7eDOyddWtr0Uuht8F4M0nu6tqsTLU48CDwHHBWPud6krNpcbLteZKWIQn1XpyPHwOc3OV6z5FUPyC90c/aniOpKGr8NmDd1hwbad3YmrntNbZnAeR1ZasBlw11wXwz7Amw2zIbsXXoLQZBMFpUyFq0fQRJ+agTndaItK9z+A+S2PqbgDWAP0i61PbjpTvRgZ4fkZHenGMKQsGvsr0/MMfzZUla4r0t6kgYFO0+L2qcvzW0riHgM4U+rW67VQetk6jwkBRFg8OJBUEwqjQXWpxFqiLS4iWkkVeRjwCnOXEPcB8N6M/2gyO7ANhB0koAkpaX9LIyDbPQ8COStsiHPghcPESTspxL0pKckvv0SkndVno+AURRzSAIeovmshavBdaUtHpO4HgfcEbbOX8hRdnIeQ2vAu6t+xJ6PrRo+zZJ+wHnSZoEzCFlvpTlw8DhOfX9XtI3grocSQoZ3pAnLf8FbNelzRHA7yU9EPNkQRD0DA0tiLY9V9KnSV/0JwNH2b5V0l75+cNJAu1H56kakfIXHqp77RAN7jF+1YBo8I7Hv6V2P5qQqNp5g31qtV+mUiWdzjQhMPuvgfpFvZedtFit9ts8W6ZU3tCss8SjtW0sPu25Wu2/2iMSVSvVz/jmb653Xzw4r76I84Wz/lBfNPj4b5QXDd7lmz0pGtzzI7IgCIJgBAmJqiAIgqCvGQcSVeHIeoxzFqkfbjinZh0wqK9cD3Di9T+o1X6/6fvW7sOkBkKLT6l+3agVqRfKOn+xeiE9gCvn1c81eu6xepHveTxduw/LNhByXs7160xcPbdWxjgbT1mpdh8aIUZkQRAEQV8zDvIkwpEFQRBMZMbBiGzM1pFJmj3MdiOuIl9GDX84/chajGd1PzMIgmCUqCBR1av0w4LodvahN1Tk96E3+hEEQTBsPODSW68y5o5M0jRJF0i6QdLNkrbNx5eU9H9Z3f4WSTuXVZEvjvYk7SDp6Pz4aEmHS7pU0l2StsnHF5d0QlayP5Gk59hq/9MsjnmrpG/mYwv1Q9LbJF2ZX8fJkqbl42/PSviXAe9p9M0LgiCoS4P1yMaKXpgjewbY3vbjklYArpJ0BvB24O+2/xNSQTbbj0n6PElFfrirwVcD3kgSrLxQ0iuATwBP2V5X0rrADYXz97X9sFKtnQskrWv7sGI/cr/3IynuPynpK8DnJX0X+DlJIPMe4MRh9jkIgmBkGAfp92M+IiPJlPyPpJnA+STZ/xeSlOffkmuDbZF1E5vgJNsDtu8mSVatBWwJHAtgeyYws3D+TpJuAG4E1iGVZ2lnk3z88qx4/2HgZdn2fbbvzkLEx3bqULE0wj2z72/gJQZBEJRkwOW3HqUXRmS7AisCG+RyKfcDU23fJWkD4J3AdySdZ/uAoQwVKL7j7do+7X8ND3KcXGPsi8CGth/JIcpOWkEC/mB7l7b263Wyu1BnC6UR3v+y7Xv3bgmCYPwxt3eTOMrSCyOyZYAHsxPbmjSSQdLKpHDfscAhwOvz+WVU5P8p6dVZZHj7tud2lDRJ0hqkwpd3kgpz7pqv+xpg3Xzu0qSSMI9lpeZ3FOwU+3EVsFkOUyJpCUmvBO4AVs/XAljA0QVBEIw5dvmtR+mFEdlxwJmSriMVXLsjH38t8D1JAyTF+0/k42VU5L9KKrr5V+AWUkntFneSSrm8ENjL9jOSfgr8Moc3ZwDXANi+SdKNwK2kMOTlBTsL9EPSbsDxklrqsPvlUeWewP9JeohUYPM11d6eIAiCEaSHkzjKMmaOzPa0/PshYNMOp9xPKgfQ3u6HwA+72D4FOGWQpy+3/bm2858m1c7pZGu3QY4v0A/bfwQ27HDeOTRQOC4IgmBE6OG5r7L0wogsCIIgGCvGQdZiXzsySVcD7YWePmj75k7nDza6CoIgmLDEiGxssb3xWPehaZZQfVXued0TJbuyWAN5QHXV67993YG1+zDnlENr29j7oL/XtvFnP1Wr/fINFIL8h5+tbaNuodKpDXzk/LuB17Hp3HqFTgGm1SyW+kDNwpxN4bn1qzuMNX3tyIIgCIKajIPQYi+k348INUSJt5PUadFzEATB+GMcLIget46sBtvRWb1jUCTFyDYIgv5kHGgtTghHJulLkq7NosDfLBz/UD52k6RfS3oD8G7S+rUZktYolnSRtEJWHkHSblkc+EzgvCxyfFS+zo0F8eN1JF2T7c2UtObovwNBEASDMA5GZON+JCHpbcCawEYkKakzJG0J/BvYF9gsC/8un8WBzwDOymvRkIac3N4UWDe3+x/gj7Z3l7QscI2k84G9gENtHydpUaB+NkcQBEFTjIM5snHvyIC35e3GvD+N5NheB5zSUtG3/fAwbP+h0O5twLslfTHvTwVWBa4E9pX0EuC0LFa8AFn9Y0+AzZZfn7WWevkwuhIEQVCd8ZC1OBFCiwK+Y3u9vL3C9i/y8TJj5bnMf5/aBYOfbLvOewvXWdX27bZ/QwpXPg2cK+lN7RewfYTt6banhxMLgmBUGQehxYngyM4Fdi8UulxF0krABaQSLS/Ix5fP57eLEt8PbJAf79DlOp9RjkVKWj//fjlwr+3DgDOYL0gcBEEw9jToyHIh4Tsl3SPpq4Ocs1XOGbhV0sVNvIRx78hsnwf8BrhS0s0kDcalbN8KHAhcLOkm4Pu5yQnAl3LCxhok5f1PSLoCWGGIS30LmALMlHRL3gfYGbgl1ylbC/hVoy8wCIKgDh4ovw1BLj78Y1KVkLWBXdqXMuX8gZ8A77a9DrBjEy9h3M6RtUSJ8+NDgYUkHmwfAxzTduxyFk6/L46i9svnHQ0cXWj3NPDxDtf4DvCdqv0PgiAYFZoLGW4E3GP7XgBJJwDbArcVznk/KVfgLwC2H2ziwuN+RBYEQRAMjucOlN6K1ezztmfB1Cqk0lktZuVjRV4JLJeXNV0v6UNNvIZxOyILgiAISlBhoXOxmn0HOq1Vah/uLULKOXgzsDhpyucq23eV7kQHwpH1GE+6ftnxpTWlto0mhIcn1RSYbULwd8oOe9e28dxBX6ptY2rN5YNzGvh71BX8BVhM9YI4zzawZmnJBu7vJoJpc10vbX1yrwTEmgstzgJeWth/CdCuuD0LeMj2k8CTki4hLYWq5ch65J0MgiAIxoTmshavBdaUtHoWf3gfKVO7yO+ALSQtImkJYGPg9rovIUZkQRAEExi7mRGZ7bmSPk1aijQZOMr2rZL2ys8fbvt2SecAM4EB4Ejbt9S99rh3ZJL2B2bbPqSmnWWB99v+Sd5fGTjM9lBry4IgCHqbBhc62z4bOLvt2OFt+98DvtfYRYnQ4gJ0UbFfFvhka8f238OJBUHQ71TJWuxVxqUjk7RvXl1+PvCqfKysiv00SRdIukHSzS0Ve+AgYI28Iv17klbLC5+RNFXSL/P5N0raumD7NEnnSLpb0ndH+a0IgiAYmnEgUTXuQouSNiBNMq5Pen03ANd3aVZUsV8E2N7245JWAK7KivhfBV5je718ndUK7T8FYPu1ktYiOcRX5ufWy315FrhT0g9tF9daBEEQjB29O9AqzXgckW0BnG77KduPs3DWTCeKKvYC/kfSTOB80oK+F3ZpvznwawDbdwB/Ji38A7jA9mO2nyGtcH9Ze+PiIsN7Zt9fortBEATN4AGX3nqV8ejIoPMykbIq9rsCKwIb5NHXPzuc385QC3SeLTyeR4dRcFH9/hXTVutyqSAIggYZB6HF8ejILgG2l7S4pKWAd+Xj91NOxX4Z4EHbc/JcV2sE1a6K337NXQFySHFV4M5hv4IgCILRYqDC1qOMO0dm+wbgRGAGcCpwaX6qrIr9ccB0SdeRnNMd2e6/gcsl3SKpPXX0J8DkrK5/IrCb7WcJgiDocTzXpbdeZdwlewDYPpBUoqWdMir2D5GSPzrZfX/bodfk488Au3U4v932Nl07HwRBMIr08txXWcalIwuCIAhK0sMhw7KEIwuCIJjANKDjPOaEI+sx/jXwdG0bcybVvzPnNHB3P6V66uB7H9QunF2dJpTrj7yuvprOp6d/pVb7vzdwXywxpHBNOebV1OVrQoH/mZqq8wAnTPp3bRsvm7x0rfZuRIO/AcKRBUEQBP1MA5WjxpxwZEEQBBOY8RBaHHfp9yOBpAMkvWUY7daTdKWkWyXNlLTzSPQvCIJguHig/NarxIisC5Im2/76MJs/BXzI9t257Mv1ks61/WhzPQyCIBg+veygyjKhR2RZwf4OScfkEdMpkpaQdL+kr0u6DNhR0tGSdshtNpR0haSbJF0jaSlJk7Mi/rXZzscBbN9l++78+O/AgyT5qyAIgt7AKr/1KDEiS2Ve9rB9uaSjmF9z7BnbmwNIenv+vShJuWNn29dKWhp4GtgDeMz2hpIWIymAnGf7vtZFJG0ELAr8adReWRAEQRdiRDY++Kvty/PjY0lK9pAcVjuvAh6wfS2A7cdtzwXeBnxI0gzgauAFwJqtRpJeTFLH/4i98G1TVL//2+xZDb2sIAiC7gzMVemtV4kR2cJK+a39J9tPJKncd1r8IeAzts9d6Ik0avs/YD/bV3XsgH0EcATAW176Hz2yuCQIgomAezhkWJYYkcGqklrairsAlw1x7h3AypI2BMjzY4sA55IEiafk46+UtGQORZ4O/Mr2ySP3EoIgCIbHeMhaDEcGtwMfzoU0lwd+OtiJtp8DdgZ+KOkm4A+kWmVHkopm3iDpFuBnpNHuTsCWwG6SZuRtvZF8MUEQBFXwgEpvvUqEFmHA9l5tx1Yr7tjerfD4WmCTDnb+K29Fjs1bEARBT1JTdawnCEcWBEEwgenlkVZZJrQjs30/uaZYr/DiSUvWtjGvATHSJSdNqW1jRRat1f7Pfqp2H6YyubaNuoK/AD+67uBa7T+ywRdr9+Ep6ovqPVfz6/tykxar3YcmRhBLT6p3bwI8XVOk8AnPqd2HJhiY1/+OLObIgiAIJjBNzpFJerukOyXdI+mrQ5y3oaR5LaGJukzoEVkQBMFEp6n0e0mTgR8DbwVmAddKOsP2bR3OO5iU7d0I49qRSdofmA0sDVxi+/wx6sd6wMq2zx6L6wdBEAxGg2n1GwH32L4XQNIJwLakjO4inwFOBTZs6sITIrRo++tj5cQy6wHvHMPrB0EQdGTAKr0VVYjytmfB1CrAXwv7s/Kx55G0CrA9cHiTr2HcOTJJ++YY7fkkSSnaRH8PknRbFvc9JB97oaTTsxDwTZLekI9/XtItedsnH1strxVrXe+LeeSHpIskHZzFhO+StEVeFH0AsHNeRxalXIIg6BkG5k0qvdk+wvb0wnZEwVSnGGV7as4PgK/YDZT5LjCuQouSNgDeB6xPem03ANcXnl+e9G1gLduWtGx+6jDgYtvb5/jttGzrI8DGpD/Q1ZIuBh7p0o1FbG8k6Z3AN2y/RdLXgem2P93Yiw2CIGiABteRzQJeWth/CfD3tnOmAydIAlgBeKekubZ/W+fC421EtgVwuu2nbD8OnNH2/OPAM8CRkt5DqhcG8CayooftebYfI4kHn277SduzgdOy/W6cln9fT9vC6sEoDtfvnn1f9wZBEAQN0WDW4rXAmpJWz5Go99H2GWx7ddur2V4NOAX4ZF0nBuPPkUFnUd/0RFKq34g00bgdcM4Qdgb7q81lwfdtatvzz+bf8yg54i0O19ectnqZJkEQBI1QZY5sKPLn66dJ2Yi3AyfZvlXSXpLa1ZMaZbw5skuA7SUtLmkp4F3FJyVNA5bJ2YP7kJIwAC4APpHPmZwV6y8BtsuFNpckhSQvBf4JrCTpBbn22DYl+vUEsFTdFxcEQdA0tkpv3W35bNuvtL2G7QPzscNtL5TcYXs326c08RrGlSOzfQOpjtgM0qjr0rZTlgLOygLBFwOfy8f3BraWdDMpJLhOtnU0cA2pxtiRtm+0PYeUvHE1cBZJEb8bFwJrR7JHEAS9hl1+61XGVbIHQP4WcOAQp2zUoc0/Sesd2o9/H/h+h+OHkRJE2o9vVXj8EHmOzPbDNLhmIgiCoCnmDfT/eGbcObIgCIKgPL080ipLOLIgCIIJTLckjn4gHFmPccTXX17bxu/3bV+6UZ15qn9zn7/Yc7XaL6/6CuVzGqgE8PeBp2vbqKte/8vrD6ndh4vW+VptG/dPqVcV4edz/9r9pC5suejKtW08Sn3l+bUG2hOWq3HK3Fm1+9AETWktjiXhyIIgCCYwMSILgiAI+ppxMEXWH+n3kq4YZrvtJK3dUB+2amkw5v29JH2oCdtBEARjxbyBSaW3XqUvRmS239D9rI5sR1rr1V5GoCOSFsmr0zuxFakkzBW5T42qNwdBEIwFzVVxGTv6wpFJmm17mqStgP2Bh4DXkBYvfyALAB8EvJskIXUeSfPw3cAbJe0HvNf2nzrYvojknDYDzpB0F7AfsCjwb2BXYHFgL2CepA+Q6um8GZht+5Bs42pga2BZYA/bl0pagrSoei2SZMtqwKdsX9fcuxMEQTB8PKgaX//QF46sjfWBdUiqypcDm0m6jTZVe9uPSjoDOKuEDMqytt8IIGk5YJNs56PAl21/QdLhZMeVz3tzm40FVO+BtwCfBB6xva6k15AUR4IgCHqGgXEwSda7Qc/Bucb2LNsDJMewGoOr2pflxMLjlwDnZrmqL5GcZhk6qd5vDpwAYPsWYGanhkX1+19c3PGUIAiCEWEAld56lX50ZM8WHs8jjYSqqNp34snC4x8CP7L9WuDjLKxu361fRdX7Un/5ovr9Hm9ct+TlgiAI6jMPld56lX4MLS5EVrVfwvbZkq4C7slPDUd1fhngb/nxhwvHnwCWrmjrMmAn4MKcPfnaiu2DIAhGlPEwR9aPI7JODKZqfwLwJUk3SlqjpK39gZMlXUpKKmlxJqlEzAxJZQpsAvwEWDH36yuk0OJjJdsGQRCMOAMVtl6lL0Zktqfl3xcBFxWOf7pwWidV+8uBIdeRFRXr8/7vgN91OO8uoBj3u7Tw3FaFx8+r3pPm7T5g+5nsSC8A/jxUf4IgCEaTXnZQZekLR9bHLEEKK04hzZd9wnY9AcIgCIIGGQ+hxQnjyCT9mLRWrMihtn85Ute0/QQwfaTsB0EQ1GWg//3YxHFktj811n0ow7Hf+EdtGxtMebL7SV2YuuhgAifluXJe1TybBfmHn+1+UhcmN/BtcwnV/zd5inrvZxPK9Vvd+p3aNubdc2299jueWbsPF3p2bRurafHaNs4aqPe/uv6iK9XuQxP0cjZiWSaMIwuCIAgWJubIgiAIgr5moIHag2NNz6bf94Li/RDXOHKkrxEEQTAauMLWq/SsI6upeF/ayUjVJ0Bsf9R2KUX9IAiCXmY8rCPrWUcmaXb+vZWkiySdIukOScdJaSws6SBJt0maKemQXC/s3cD38sLljougs73/kXQxsLekDSRdLOl6SedKerGkV0u6ptBmtbywudV+en78NklXSrpB0smSpknaSNJp+fltJT0taVFJUyXdO6JvXBAEQQXmSqW3XqVnHVkb6wP7kEZaLycp3i9PUrxfx/a6wLdtXwGcAXzJ9nqdyrYUaCneH0bSV9zB9gbAUcCBtm8HFpX08nz+zsBJRQOSViCVfHmL7dcD1wGfB27IfQbYArgF2BDYmFTuhTY7z4sGX/Lk3VXelyAIglo0GVqU9HZJd0q6R9JXOzy/ax54zJR0haTXNfEa+iXZ4xrbswAkzSApZ1zFfMX7/yMV0KxCS/H+VaTaZn/IA73JwAP5uZNIWokHkRzZzm02NiE518tz20WBK23PzX/IV5MUR74PbJltX9pmA9tHAEcA/PwlH+jlUHQQBOOMptaRSZoM/Bh4KzALuFbSGW3TMPcBb7T9iKR3kD73Nq577X5xZB0V7yVtRCpw+T7g08CbKthsLbYScKvtTTuccyJJd/E0wLbbh0sC/mB7lw5tLwXeAcwBzicV2JwMfLFCH4MgCEaUBue+NgLusX0vgKQTgG2B5x1Zjpq1uIpUNqs2/RJaXIiseL+M7bNJYcf18lNVFe/vJAn7bprtTpG0DkAOTc4D/psFa5a1uIoU5nxFbruEpFfm5y7J/brS9r+AF5AqRd9aoW9BEAQjSpXQYnEaJG97FkytAvy1sD8rHxuMPYDfN/Ea+mVE1omlgN9JmkoaGRUV738u6bOkea+h5smw/ZykHYDDJC1Dek9+wHyHcyLwPWD1Dm3/JWk34HhJi+XD+wF3kebCXkhyaJCU7x+0HaHDIAh6hiqhxeI0SAc6Wer4eSdpa5Ij27z81QenZx3ZKCvezyDNYXU69xDgkMHa2/4jKZGjvd3TwGKF/T3bzwmCIBhr6ovRPc8s4KWF/ZcAf28/SdK6wJHAO2z/u4kL921oMQiCIKiPVX7rwrXAmpJWl7QoKXfhjOIJklYFTgM+mEtjNULPjsiaYCwU7+vy0OT6NmY/u2htG8st/1RtG889Vi+K2oTg72Kq/11tXgPR4Odq2rh/ypTafagr+Asw+RULBR8qsdKchUr9VWZgsfp/j2XdwD9aTZ5h3lh3AWgu2SMn4H0aOJeU2HaU7Vsl7ZWfPxz4Oilf4Cc503uu7doVQsa1I+sXxfsgCIKxoknFjpx8d3bbscMLjz8KfLTBSwLj3JEFQRAEQzMess96Yo5stAWCs+xVZS1HSUfnDMfKSFpP0juH0zYIgmCkGFD5rVfpCUc2WgLBBbYChnvN4bIeEI4sCIKeYm6FrVfpCUc2wgLBny20O0HSasBewOdyuy3aR1qF/kjSj3L7/wNWKpyzkNBwPn6RpIMlXSPprmx/UeAAYOd8zXapqyAIgjFhPJRx6cU5svWBdUjrDy4nKWfcRhIIXsu2JS1r+1FJZwBn2T5lCHtfBVa3/Wyh3eHA7LxGDEl7DNJ2e5IW42tJi5tvA46SNIUkNLxtXhS9M3AgsHtut4jtjXIo8Ru23yLp68D0tnVwQRAEY0ovhwzL0hMjsjausT3L9gAwgyQQ/DjzBYLfA1TJDZ8JHCfpA1QfHW8JHG97nu2/A3/Mx4tCwzNIah5FzbDT8u/rc/+HpCj7cs3sUL8PgmD0iHpkI0NHgWCSiseppHmxcyrY+0+SIvMGwPXqXEhzLvm9yKHM4kKsTiPqltDwenl7re23dXgN8ygx6rV9hO3ptqdvNG3N7q8oCIKgIcZDaLEXHdlCDFcgWNIk4KW2LwS+DCwLTOvQ7n6So4Ok1txafXoJ8D5Jk/Mc2Nb5+KBCw0NQVcw4CIJgxJmLS2+9Sl84MpIDOEupQvPFLCgQ/CVJNw6S7DEZOFbSzcCNwP+z/ShwJrB9K9kD+DnwRqWK0Bszv8TL6cDdwM3AT/O1sf0csANwsKSbSCHQblmQFwJrR7JHEAS9xHgYkfVEssdICQTbnkMHdeWs8bVu2+FNCo+/ls8zqc5ZJ9sz6CA03CYo/BB5jsz2w3QQFw6CIBhLennuqyw94ciCIAiCsWE8ZC2OG0fWjwLBQRAEY81ATwcNyzFuHNl4EQi+1A/XtnHtYvX/rFMeXbK2jXk8Xav91AZuz2ddP3DShAr/cpMW637SEPx87l+7n9SFeTueWdtGXfX6d93y7dp9OHaDfWrbeFD1dSpePHlarfZzeqTGbm/0oh7jxpEFQRAE1enlbMSyhCMLgiCYwPS/G+vh9PvRVsQPgiCYiISyxwgyBor4tZA09iVngyAIKjKAS2+9Ss86shFWxH+FpPMl3STpBklrZKX770m6RdLNrUXL+fpnFdr+SNJu+fH9kr4u6TJgx3al/XzOkpKOknRtXri97Ui+b0EQBFWIBdGjR9OK+McBB9k+XdJUkkN/D0n66nXACsC1ki4p0bdnbG8OIOnvFJT28/P7An+0vXs+do2k820/2dlcEATB6NHLIcOy9OyIrI3GFPElLQWsYvt0ANvP2H6KpADSUrr/J0mOqowSx4mFx52U9t8GfDWr5F8ETAVWbevT8+r3f5n9lzIvIwiCoBHm4dJbr9IvjqxJRfzBFgUNdvx5ZfzM1LbniyOrTkr7At5bUMpf1fbtRQNF9ftVpy3g44IgCEaUmCMbQ4ariG/7cWCWpO2yncUkLUFSut85K92vSNJRvAb4M0nsdzFJywBvHqQ/gyntnwt8pjCvt36Nlx0EQdAoMUc2tiwF/C7PcYkFFfF/LumzwA62/9Sh7QeBn0k6AJgD7EhSut8UuIn0N/uy7X8ASDqJFDa8m6Si34mW0v4yuT//L8/ZfQv4ATAzO7P7gW3qvPAgCIKm6OWRVll61pGNlCJ+Pudu4E0dnvpS3trP/zJplNV+fLXC48GU9p8GPj5Uf4IgCMaKSPYIgiAI+pomkz0kvV3SnZLukfTVDs9L0mH5+ZmSXt/Ea+jZEVkT9KMi/lqTlq5t46+uJ9YLMKWB7zjLakr3k4bg3362+0ldWLJmHwCe8bzaNurqw2656Mq1+3ChZ9e2MbBYvRfShODvidf/oLaN9dbZpbaNF01Zplb7f819onYfmsANhRazKMSPgbcCs0hLmM6wfVvhtHcAa+ZtY1LB4o3rXntcO7LxoogfBEEwUjQYWtwIuMf2vQBZFGJboOjItgV+lYsWXyVpWUkvtv1AnQtHaDEIgmACM2CX3oprXvO2Z8HUKkCx3tCsfIyK51RmXI/IgiAIgqGpEli0fQRwxCBPd1qL226+zDmVGVcjsqyLWElsuKXpOIxrHRkq+0EQ9DsNLoieBby0sP8Skqxg1XMqM64cGbAVMFzV/ErY/mjbJGYQBEHf0WDW4rXAmpJWl7Qo8D7gjLZzzgA+lLMXNwEeqzs/Bn3iyCR9KKdq3iTp15LeJenqrCZ/vqQXSloN2Av4XFa+32IQW6tLujKr0X+r7bkv5eMzJX0zH1tS0v/la99SUMW/SNL0/HgPSXflYz+X9KN8/OicanqFpHsl7TCCb1MQBEFlmhqRZdnAT5PUjG4HTrJ9q6S9JO2VTzsbuBe4B/g58MkmXkPPz5FJWoekIL+Z7YckLU+KqW6SVe8/SlLh+IKkw4HZtg8ZwuShwE9t/0rS81mNkt5GSgndiBTHPUPSlsCKwN9t/2c+b4GcW0krA/8NvJ4kj/VHkjpIixeTFkqvRfo2spAqf54w3RPgzctPZ92lOlafCYIgaJym0u8BsmTg2W3HDi88NtB4Nnk/jMjeBJxi+yEA2w+T4qrnSrqZpMSxTgV7mwHH58e/Lhx/W95uBG4gOZ41gZuBt0g6WNIWth9rs7cRcLHth7O6x8ltz//W9kAOQ76wU4eKosHhxIIgGE2iQvToIBbOavkh8CPbryXJP7Ur0nej01cQAd8pqNS/wvYvbN9FUrO/GfiOpK93aDcUxVW93c4NgiAYVWyX3nqVfnBkFwA7SXoBQA4tLgP8LT//4cK5QyrfZy4nTUIC7Fo4fi6we1bVR9IqklbKocOnbB8LHEIKIRa5BnijpOVy2Zb3Vnp1QRAEY8h4KOPS83NkebLwQOBiSfNIob/9gZMl/Q24Clg9n34mcIqkbYHP2L60g8m9gd9I2ptUy6x1nfMkvRq4MldcmQ18AHgF8D1JAySl/E+09e9vkv4HuJqURnob0B5+DIIg6El6uWBmWXrekQHYPgY4pu3w7zqcdxewbhdb95HKtbQ4qPDcoaRkkCJ/Io3W2u1sVdj9je0j8ojsdOC8fM5ubW2mDdW3IAiC0aaXR1pl6QtH1gfsL+ktpLm684Dfjm13giAIytHLc19lGbeOTNK+pIKZRU62fWDT17L9xaZsPcrc2jZW1xK1bcxt4Fvacp5cq/2mcxer3Ycm/kVPmPTv2jaWnrRorfaPMqd2H1bT4rVtLFvzb/qg6t/fTSjXz7j1+O4ndWHdtd/X/aQhuPvRv3U/aRTo5WzEsoxbR5YdVuNOKwiCYDzR5DqysWLcOrIgCIKgO/Pc/2OyRtPvR1O0d7Rt1kXSPlIDMb8gCIIGGQ/p902vI9uKURLt7UP2AcKRBUHQU7jCT69SypGNsWjvwZI+WThnf0lfGOz8NnuS9L0s9ntzQfB3K0mXSDpd0m2SDpc0KT83O1/z+vzaNspiwPdKenc+Z3K227r2xwt2L5J0iqQ7JB2X+/BZYGXgQkkXlnnPgyAIRoMqhTV7la6OrCDa+ybbryMtKL6MJNq7PnACSbT3fuBw4P9liadOi5FhvmjvhsA/CtcpivauB2yQRXtPAHYutN+JtBh6sPOLvCc/9zrgLaSFzS/Oz20EfAF4LbBGPhdgSeAi2xuQlEK+DbwV2B44IJ+zB6n8wIbAhsDHJLUWZa9PGn2tDbycJHZ8GGmx9Na2tx7kfQmCIBh1XGHrVcqMyMZUtNf2jcBKklaW9DrgEdt/Gez8tmttDhxve57tfwIXkxwPwDW277U9L/dn83z8OeCc/PhmkiDwnPx4tUJfPyRpBknR4wWFa19je5btAWBGoc2gqFA+/M4n7u12ehAEQWOMhzmyMlmLg4n2ft/2GZK2IklGVWEo0d6fdXjuFGAH4EWkEVq384s2y/ahtT/H81cIDpBFf20PZOWOlt3P2F5A8SO/F0WR4HmUeI+L5cN3X22H3r1bgiAYd0yUrMUxFe3Nz52Q2+zA/HpeQ53f4hJg5zyntSKwJUnkF2CjPF83iRS6vKxLv4ucC3xC0pR87VdKWrJLmzLvTRAEwagyHkZkXR2Z7VtJC4svlnQT8H3mi/ZeCjxUOP1MYPuhkj1Ic2yfknQtySG2rnMe8BuSaO/NJIe1VKEPSwF/a5XFHur8AqcDM0mFLv9ImstrzctdSdJZvAW4L59bliNJ4sA3SLoF+BndR15HAL+PZI8gCHqJ8ZC1qPGgs1WVHAL8ou1txrgrC9FEaHEFptTuRy9IVL10bv3ybeNFomoZ1f+bvqCB+6IXJKrOe/q+2jbGi0TV3Of+VvufZPqLtyj9b3LdA5f2ZE3FUPYIgiCYwPRyyLAsI+bIRlO0tyq2LwIuGuNudGT9eVWLXS/M+Xq0to2pqn9rXD338Vrtp02qLxo81/Nq23jZ5KVr23ja9UYiaw3Uvy/OGvhH95NGmBdPrl/J6EVTlul+UhfqjqYAZt52QveThmD9dd5fuw9NMB6SPUbMkYVobxAEQe/Ty3NfZYnQYhAEwQSmlxU7yhKOLAiCYAIzHkZkTYsG9wxllPglHS1ph4o2zxrkuZ5T3A+CIOjGhNBa7GO2IpT4gyAIhmS01pFJWl7SHyTdnX8v1+Gcl0q6UNLtkm6VtHcZ233nyNSgEn9mS0lXZHX7HfI1OqrmZ5bupJqf2/2vpBskXSBpRUlrSLqh8Pyakq5v+C0JgiAYNvM8UHqryVeBC2yvSVKM+mqHc+YCX7D9amATknjG2t0M95UjGwElfoAXkwSDtyEpfcDwVfNvsP16kjjxN2z/CXhM0nr5nI8AR3d4Xc+LBl8+++7yb0gQBEFNRjG0uC1wTH58DLBd+wm2H7B9Q378BHA7sEo3w33lyGheiR/gt7YHbN8GvDAfG45q/gBwYn58bOH4kcBHJE0maTr+pr0Dto+wPd329M2mtQv4B0EQjBxVQovFL91527PCpV5YkBh8AGjXxl2AHFlbn1RhZEj6LWtxJJT4i2r1avvdicFU8wc771TgGyStx+tt19c7CoIgaAhXCBkWK3V0QtL5pCol7exbpU9ZDP5UYB/bXZUV+m1E1rQS/2AMRzV/EkmdH+D9reO2nyGp5f8U+OUw+xMEQTAiNKl+b/sttl/TYfsd8M/WFE3+/WAnG7mqyKnAcbZPK/Ma+sqRjYAS/2AMRzX/SWCdnMzxJuZXkwY4jjRCO69iP4IgCEYU26W3mpzB/MHGh4HftZ8gScAvgNttf7+s4X4LLWL7GOZPGLZY6A2xfRewbhdbu7XtT8u/TZpv+1Lb8xcxiEZjqy3w3x2e3hw4Ks+rBUEQ9AyjqLV4EHCSpD2Av5C1eCWtDBxp+53AZsAHgZslzcjt/sv22UMZ7jtH1m9IOp2U3fimse5LEARBO6O10DnnB7y5w/G/A+/Mjy9j6ByFjkwIRzaWSvy2t69y/jFz/1z7misuUl9h/PGBZ7uf1IWNpwyZlNSVB/xM7T5MbiB63oSEzxOeU6v9KXNn1e7D+ovW+3sAPEO9oMKcBj40/zX3ido2mqgFVle9/sZbF0pgHhPGg0TVhHBkocQfBEHQmfFQXHlCOLIgCIKgM+OhsOaYZS2WFPXdS9KH8uPd8qRgU9dfsSBttYWkTzZlOwiCoF+YNzBQeutVxjL9fiu6iPraPtz2r/LubkBjjow06XhHlrb6K9C4I8tqHkEQBD3LKKbfjxiNhxbzCOqLpHVTM4GTgP2ARYF/A7sCi5NEfedJ+gDwmU56iJL2B2YD9wPTgeMkPQ1savvpDucfBLybJDx5nu0vSnoZcBSwIvAvkt7h8sB3gcVziuedwBr58R9IuonnZLWQ04FHbO+e00ZXt72fpN8CLwWmAofmFe+tci7fB/4D+EKWWflsfv1XA5+MNPwgCHqF8RBabNSRFUR9N7P9UFbeMEnU15I+Slpc/AVJhwOzbR/Sza7tUyR9Gvii7esGufbywPbAWvlay+anfgT8yvYxknYHDrO9naSvA9Ntfzo7m3Vsr5dtvQ/YgrSAbxWSsDCk9WAn5Me7235Y0uLAtZJOzemlSwK32P66pFcDX8nvxxxJPyE58tYoMwiCYEzp5ZFWWZoOLY6EqG9ZHgeeAY6U9B7gqXx8U+YL9f6a+WK+Q3EpsEUuH3Ab86VVNgWuyOd8NquLXEUambXUfueR5FUghS83IDm6GXn/5e0XKwpx/uupf7Q/HQRBMGJEYc2FGUzU90e2Xwt8nBSKaxzbc0klVk4llQc4Z7BTS9j6G7Ac8HaS7uKlwE6kEeQTWZz4LaQQ5+uAG5n/up4phA4FHJNLyaxn+1W29+9wvefV71dcopPeZhAEwcgwWoU1R5KmHdlIivoOeX5WS14mS5nsQ6onBmkE9b78eFfmi/x2s31lttNyZF/MvyG9pkdsPyVpLVIBuE5cAOwgaaXcx+XznF0QBEFPEFmLbYywqO/RwOH5/MU7PL8UcJakmaT6YZ/Lxz9Lqgc2k6ThtVDp7Dy3dXmuCP29fPhSYBHb9wA3kBJEWo7sHGCRbPNbpPDiQuQaZ/sB5+Vz/8D8+bYgCIIxZzyMyDQeJvrGE9NfvEXtP0gTElXPeG5tG+sv8oJa7XtFomqKKku/LcS/Buq9lofmPlm7D+NFomrmsw/UtnHbw3+pbWOt5V5aq30TElVTVnh57Ztz0cVeUvqP8tyzs+r/M4wAoewRBEEwgRkPg5mecGRVRX3z2q7V2w5/xfa5I9G/IAiC8Ur/uzGqreqOrTc2YM+xbD+ebPRCH3rFRi/0oVds9EIfmrIxEba+qhAdPM+eY9x+PNnohT70io1e6EOv2OiFPjRlY9wTjiwIgiDoa8KRBUEQBH1NOLL+5Igxbj+ebPRCH3rFRi/0oVds9EIfmrIx7ol1ZEEQBEFfEyOyIAiCoK8JRxYEQRD0NeHIgiAIgr4mHFkQjAGS2pVsOh7rF5p4PZIWl/Sq5noVTBQi2aNPkPT6DocfA/5sl1P4lbQk8LTtAUmvBNYCfm97Tsn2F9h+c7djg7Q9kyHUcGy/u0wfsq01gFm2n8214dYlVQF/tIKNVYCXUZBps31JybZLAyva/lPb8XVtzyxp4wbbr+92rISdJu6LP5EqOFwKXOJUtaESdV+PpHcBhwCL2l5d0nrAAVXui7rUub8L518H/BL4je1HhtkPkUpOvdz2AZJWBV5k+5rh2JsI9ITWYlCKnwCvB2aSCna+Jj9+gaS9bJ9XwsYlpMrXy5FqpV0H7Ez6pxkUSVOBJYAVctuWAvbSwMol+39I/v0e4EXAsXl/F+D+kjZanApMl/QK4BfAGaQq4O8s01jSwaTXfRs8L+du0vvTre1OwA+AByVNAXazfW1++mjS32io9u/I/VxF0mGFp5YGhlNyoIn7Ym1gY2AL4JBcY+8m29t3a9jg69mfVBj3IgDbMyStVqE9uTL8wcBKpPdCyZSX7tKuifu7xfuAj5Cqwrec2nmuNmL4CTAAvAk4gFQv8VRgw4p9mTCEI+sf7gf2cKr5hqS1gS+R6qGdBpT5wJJTMdA9gB/a/q6kG0u0+zipyOjKwPXM/0d/HPhxmc7bvjj3+1u2tyw8daakUiOhAgO250raHviB7R+WfB0ttgNeZfvZitcF+C9gA9sPSNoI+LWk/7J9GvPfl6H4O+kLxLtJ72WLJ5hfQ68K91P/vpgHzMm/B4B/Ag+WvH5Tr2eu7cdUr2TOd4F32b69Yrva93cLp/qF+0r6b2Ab4ChgQNJRwKG2Hy5hZmPbr2/d07YfkbRolX5MNMKR9Q9rtT6sIBXtlLS+7Xsr/PNL0qakEdge+VjXe8D2ocChkj5j+4dVO97GipJebvve3KHVgRUr2pgjaRdSxfF35WNTKrS/N58/HEc22fYDALavkbQ1qaDrSyghJG77JuAmSb8pG9LtQhP3xePAzaRCuD93KjRbigZfzy2S3g9MlrQmqSDuFRVt/HMYTqzp+xtJ65JGZe8kjaSOAzYH/sj8yvVDMUfSZPL9JGlF0heMYBDCkfUPd0r6KXBC3t8ZuEvSYqRv02XYB/gacLrtWyW9HLiwbAfyyOcNwGosOLf0q7I2SN/SL5J0b95fjfSNuAofAfYCDrR9X3aGx3ZpU+QpYIakCyg4M9ufLdH2CUlrtObH8shsK+C3wDoV+rCRpP2ZP0/XCoO9vIINaOa+2IX0QftJ4KOSriDNlV1QoR91X89ngH1Jf4/jgXNJo8oqXCfpRNLfovh3Pa1M4ybub0nXA4+SQt5fLYz6r5a0WUkzhwGnAytJOhDYgVRpPhiESPboEyQtTvqg2Zz0IXEZKZb+DLCE7dmj0IdfA2sAMyjMLZV0AEU7i5ESTQDuGE6IL78fq9q+cxhtP9zpuO1jSrR9HfBkDiEVj08BdrJ9XMk+3EFy6tcz/72kymgo22nsvshzY+8gfeFZyfbiFdo28nqyrcnAkrYfr9julx0O2/buJdvXvr+L0YbhIGkSsAnwMPBm0t/0guGMNCcS4cgmEINkDj5GmuP4me1nurS/HVi74sR1u40lgM8DL7P9sRxGepXtsyrYqJ3hluccXpl37xxOWEzSy4A1bZ+fHcoitp8o2fZq2xtXveZIIOlUUsjrHlLm4qXA1d3uhzYbtV6PpN+QRtnzSM5wGeD7tr83XJvD6EMT9/cLgG+QvliY9MXigCoOXdKVtjcdbh8mIhFa7BNyWGJ/Fk4ZrxKKupc0H3V83t+ZNLH/SuDnwAe7tL+FlHH4QIVrtvNL0gdV6x91FnAyUNqR0TnDrb1i+KDkUOAxpEQJAS+V9OGy6ffZxsdItaKWJ32LfwlwOOlbdBkulPQ9UkJGMQx2Q9k+5H40cV8cBNxge17XMwen7utZ2/bjknYFzga+QrpPSjuytqzJFo8B19n+XQkTTdzfJ5CyX9+b93cFTgTeUsHGeZLeC5xWx6lOJMKR9Q+/oEPopiLrd8oYtL2lpFsHbTWfFYDbJF3Dgh9WVdb6rGF755ysge2nVT1VrVOGW5V/+P8F3tYKSyqtqTse2KCCjU+RnOnVALbvlrRShfat0cv0wjGTUq6r0MR9cSvwNUmr2t5zOKNk6r+eKTk8ux3wI9tzJFX9EJ9KClmfnPffS3pte0ja2vY+Xdo3cX8vb7s4t/dtSdtVaA8pYrEkMFfSM5RcRjCRCUfWPzxm+/c1bayYP6z+AqC00HKF/NxzJdrvX/P6AM/lMFwrI2sNqmcP1s1wm1KcW7N9V/4QrcKztp9rOVNJi1DBmdreuuL1BqOJ+6I1Sn5D3q88Sm7g9RxOGiHfBFySw7aV5siAVwBvcl4InpNgzgPeSsrK7Mb+Fa/XiQslvQ84Ke/vAPxfFQO2l2qgHxOKcGT9QxOhqC8AlykpOQhYHfikkuJH10QH2xe3zQstAUyu8iJIHxbnkMJ5xwGbkbIQq1DMcPsNKcPt2xXaXyfpF8Cv8/6uLLgGqgwXS/ovYHFJbyUlXJxZtrGkr3c6bvuAiv1o4r6oPUqW9ELgf4CVbb9DaT3bprZ/UaLtJFLq/CqFY38BqjrHVUgjmcfy/pK5P/Mkdf2yVOf+lvQE6YuMSCOqVhbtJGA2ad6sNEoLs9ckjTJb/au63nLCEMkefYKkTmnytl0pFFXIGBQpY7DKhP7z80K218ijocNdQcIn23kBKTNLwFW2H6rQdjJwru0qcw7tNhYjhQZbmX6XAD+pkj2ZP3z3AN6WbZwLHFl2TkPSFwq7U0mLZ28vm2FXsFP7vsjp9m8GLndaiLsGcLztjSrY+D1pZLev7dflEeqNtl9bsv0lbWHvyigt9N+PNHcqYEuScz0e2N/2l7q0b+T+roukjwJ7k+ZdZ5D+V66s+r8+kQhHNsGos05G0gzyvJDt9fOxm8t+WOXzm9CzOwP4oO3Hup48QuRR7DOtBInsYBez/dQw7S0GnGH7PxrsZtlrv5XkANYmheI2I0lvXVTBxrW2N5R0Y+HemGF7vZLt/xt4mpQY8WTruMspYRTtvJh0jwq4xvbfK7SdQc37O7d5D/OzFi+1/duK7W8myVFdZXs9pWUR37S9cxU7E4kILfY4kj5g+1hJn+/0vO3vV7DVcZ0MUHbB57DnhdSsnt0zwM2S/sCCH3pDrveRdJLtnfIHxUL9tr1uhT5cQMpEa63TWpzkBN4waIuhWQIonWnY5H1h+w+SbmD+KHnvKqPkzJN5pN2a+9yE+SG+MrRGop8qdo0S74mktWzfofkCyn/Nv18k6UUVwqy15j1zm5+Q5upamcF7SXqr7U8N0aydZ2w/IwlJi+XXFlUBhiAcWe+zZP7daQK46nB6OvXWydSZF2pMz440eV5pAj2zd/69zTDatjPVhcXGtmfnOZVStDnTyaRlEVXmx2rfFx0cQCvtfNWcFFRlnu3zJPHmNSRdTno9O5RtbLv08olBrr0nKRt1IdOUz5ysNe+ZeSPwmtb/mKRjKJdoUmSWpGVJCiV/kPQISdMyGIQILfYJkjazfXm3Y11snAx81lkrcBh9qDUvlG00omdXB0kH2/5Kt2NdbFwOfKb1YS9pA1LaeKmFrDmpoMVcUrJDZfX7OveFpCNyun1T86+LAK8i3RuVFplL+lCn42XD3k3Q0P19GvA523/O+y8DDrK9yzD79EbS4vBzbJfJLJ6QhCPrE9RA/ar8gbUeUGedTG0kvYY0H1PMyKqiZ3cfnUODpUJzg7yXM6uEFiVtSFr82vqm/GJgZ9ulsx+V5K62yLuXuGQtszYbjdQ1Gy55PmhQXFLnUFLxy81UUvLJDbZLj+qUCnmeY/sJSfuRytt8y3aVygjDQvNVc5YhzW+1aodtBFxRJTlJ0gEkdZUrbD/Z7fwgQos9j5Ja/RtIa8CK8yFLM7zU9zp92YYk5NouDFt6oaakbwBbkRzZ2SRtv8soP08HCy66nQrsSFLY6HbtT5DCRS+XVHQaSwGlR7YAtq/Nk/CtEcgdFUcgewMfI6XNAxyXR0ilRqtN3heSbiI55ZPcViy0BK3qAyvl/vwx729Nyh4sK9j7mbY+LcP85RFl+W/bJ0vaHPgPkozZ4cxfrD0kNe/vQ7qfUpr7SULOhyml9bcKnpZRJ5mQxIisx8mhha1IOnSHF556AjjT9t2j2Jd7SIUxbx7uPFueG3odKTX7dUrrj460/a4uTbvZvcz25l3OWQZYDvgO8NXCU0+UzY6T9CbbfxxsJFJhBDKTtM7qyby/JCnFutSosMn7Ioe/ds7bAClz8CTnhfMlbZwFfKwVts7Zgz+2PeSIbQh7U4CZtl9doc2NtteX9B3SPfqbYhZlifa17+9s54XML4J5je2ytd3a7bwI2An4IrCcY6H0oMSIrMdxKkh5saSjW3H3qrQ+5DV/0ebzT1FtRPVX4JY6/+TA07YHJM2VtDSpgGOl0iWF5ARIC06n0znpYQGc0vUfI33bRUlSaiowTdK0kh/cbySNOjo5XlNyBEJ674uSUvOgVGHOdKEG7ouCrT+TilJ+V2nt1H+TKi1XGdmt1jb32tLwLIUWFLSeDLya+eoYZfmbpJ+RskkPVlrSMKlC+9r3t1IF8e8xfy3bDyV9yfYpFWwcSYpY/JM0GtsBqKTBOdEIR9Y/PKWk4LAOC84tdZ2Qb41UGvhG92XgbEkXs+AcW+lUb5KqxrIkkeLrSenr1wzZYmGK2WlzgftI31xLoaSe/31SBuWDpFDS7ZSoJ2b7Gzkp4Pe2q37QFvklqUbV6Xl/O5JuYlWGfV8UkbQa6T3cmeRUv1yxHxdJOpeUdm7gfVSodceCobm5wJ9tz6rYh52AtwOH2H40jwqHXATdRhP3977Ahq1RmFJRzPOB0o4MeAHJmT9KKufy0HASgSYS4cj6h+NIIZ9tSOGkDwP/qmJA0q9tf7DbsSE4kOR4pgLDKr1u+5P54eGSzgGWHkaSwx5uq/mkCur3JDmrTYDzcyhqa/IorQx5RPlpqo8Yija+L+ki5quLfGSYSQlN3BdXkypmnwzs2P7elsH2p3O4tZW8coTt04dq09b+4raQXOWQue2nJD1Iek/vJjnEKnZq39/ApLZQ4r+pNirE9vYAkl5Nmuu7UNJk2y8ZZp/GPTFH1idIut72BsXsOkkX235jBRsLZLPldOmZttcu2f4629O7n9mx7ZBZdK6wZmmQTL3rbZdSr2+9jpzksH52TNe4miRTLSUKpQXDtzrXL5O0FGmN39Vl+5DbNXFfrGX7jirXbZoOIbktgKohuW+Qwsyvsv1KSSsDJ9suVZm5zv1dsPE9YF0WLJU009WWdmxDev1bkuZ0ryQphBxVp2/jmRiR9Q+tjLgHJP0nKe271Dc0SV8DWgs9W4riIineH1GhD+dLepvt8yq0adEKB04lfdjclPuwLqkUypCJGpA+cEkhtGXaki2WphBWK8GjkqaRNBaPy9/iq4ZudieF0D7ZdrzsfN9PSenhLZ7scKwMw74vCjyiJKJcWfC3Rdv866KkEd6TFeZfmwjJbQ+sT55Psv33/AWhLHXub/I1v6RUS2wz0v1daWSaeQfp3jzUFSS2JjS2Y+uDjRQ6WgZ4DWnu4Xrg3RVtfKdmH54gZbU9TVLkeAJ4vKKNE4DXFvZfAxxdsu22pLmlf+ffre0w4A0V+rAkKdyzCCkU91ngBRVfx+KkagKnkxI8PgcsXqH9jA7HZo7RffF70vzSTXl/EVLmXp17ZTvgfyqcf3Pb/qSqfSBlCEJaf9b6O5d+Twv39zP5ceX7u4kNOLjMsdjmbxFa7AOUBGk/a/v/NWBrTMtDqIOQbKdjXWxsavvKYV6/tnp+tnMSyZkflw/tAixru1TSiZICxEWkURikkd3Wtrer0IdG7gvVFPwdwu5VtjcpeW6nkNzNtksnnUj6IunefitpicXuwG88ikoyOVJwMGldnRjeWsvaC/YnGhFa7AOc6im9G6j7gdWxPARdtOianN8Cbs/pxceSQlEfIGUMVmEvSbfbfjT3bzngf12iBEp+L5+StIzrqee/yvbrCvsX5jm3suxFGknuR3ofLiDpBZamqfuC+oK/7QofrSURVQqNfknzVeOHFZKzfYiSRuLjpIXqX7f9hyo2VFO5nrSM4V22q97TxQX7a6jmgv2JRozI+gRJB5JCSO3JBVWSJIZVHkKdtfgKXahU+2oq8AnSRDakuYCfulpdtOdHDkMdG6L9SSQnXkk9v83G0aRaVVfl/Y2BD3t+Vuao0NB98Xrgh6Tw5C1kwV9XyCaV9MvC7lySOsXPXXIxcM46faB1HyhVEX+h7fvL9qEuWli5fmfgT66gXC/pcpdMLunQdjPgb8BBQDE5pPSC/YlKOLI+YRBnUtWJtEJIM4CNbT/bRAipKpIWJS14HSCJy1YSQ80jn61sP5L3lwcudvkijh/udNx21yrZBRu3k771txZRr0oaWQ4kU0OHgSR9l7QM4GlSxezXAfvYPnaodh3s1LovWuFJkiMbluBvtlNL1FrSdaR5zufy/qKkQp8bDt1yoUSThSgb1pN0Kwsq17fm6bquLyyMSN8IvIikXF9ci9Z1oXwhA3XUtDLHCxFa7BNsD1n2XdKHS3wQD6s8hBoShs22/pMkqfQn0ofm6pI+bvv3ZW2QMiCvkHQK6QNsJ9IaoFJUcVhD8Paa7d9m+8uStgdmkfQiLySFXEtT977I4clt8zzbrVWu3cYPWTjjstOxwVik+IXGqS5YqbVczgv9lcR2/0HSaBSwKyUUXwrcSfpC0lJKeSlQdlRaVHp5iqSg/3wXKaf4MiePbFeRdFj7k1UiBhONGJGNE6p+i1OF8hBtYaN2XGZuqmDrDmAb2/fk/TWA/7O9Vlkbud3apLk9ARfYvq1C21rq+U0g6Vbb60j6OXCq7XMk3dQ279bEdbreF3XCk5ovXrwPC87VLQ1sX/b1KBVJ/aHtM/L+tqREliqVw6+2vXG3Y0O0v5gFles3JM0hPwXNVImQ9DXb3xnkuRXI8lrA19ufb+gL2LgkRmTjhyF1+nKYZKbt18DzWn2lsP2Rmn0r8mDLiWXuJclEVWV50jqlX0paUdLqtu8r2XZY6vkNc2Z26k8Dn8zrpkrPE1agjH5jq6p1sbCnKVeQclFgGumzpDj6eZwKhTVJyS/HSfpR3p8FlFWcaTFP0q6kJR4mZZLOG7rJAizkPEaAHUkZlQvhVJX7hJzIVCVxaMITI7JxQslv3scBX3MFVfO29ssA32B+osbFwAFVsv8k/ZSkbXgS6cNmR1JI53IoPZdQS8FhEJtd1fObJmdbPp7De0uQ5Lr+0fA1as+3lAlbS3qZ5xeTnARMs/34UG0GsTON9Ln0xDD6sBpwKGkxskn31D5VEkbUkHL9EPa7JiVJegkpLNt6HZcBe7u69uSEoZIGWNDTlPnm/WLgVkkXSDqjtVW4xlGkRaI75e1x0oLkKkwlqXq3ypD8izQaehdpcW8ZtgfeTQ6DOakflJ4LkfT6wjZd0l5V2tdB0pvy7/cAWwPb5sdvZ/7IqNFLNmBj7xLnfEfS0krlaG4D7pRURbAXANuz251Y2T7Yvt/2trZXsL2i7e2KTkxJ4WZQlGSyriF9udqJJOpcZVRZhjIjh18CZ5BErVcBzqT6/9mEIkKL44cy2WHfrHmNNWy/t2gvZ0CWpqEw5XO2LamVXbZkxfb/y/wPlFaq+I4N9KsMWzK/DIzJC2YLv0snzkBKW28PqbYda2L9URlnuLbtx3No72xS+vj1JP3EJmjCIQ8a1ss0IZPVjTKvYyXbRcd1tKR9GuzDuCMcWZ+gBasAt3gMuN72DNuf7maj27yYpCttbzrEKU9L2tz2Zfn8zUhzPKXJiSOdEi1KJ4wAJynVnVpW0sdICg4/L3Ht1nt4FvOdB/nxNqTSLiPNE7kft3Tow3A4lYUzA08BNoCkSj9Mu0XK9G2KUjHM7YAf2Z7T+qLREE3Y6uZEaivXl1iGcHIJM/+S9AHmr2fbJfclGIRwZP3D9Lydmff/E7iWpHJxsu3vNnCNbsK7ewG/ynNlAI+QtAqrcFbb9banxBKAIh6+gkMrfPgq0jzI70gfbu8iLcweDaY10Qc1J6Bc6nIlzvkZaWR7E3CJUtXpynNkNfvQjW7O8BzNr6kGaUH02RWvMeQyBNv/U8LG7sCPmJ8Fenk+FgxCJHv0Cfkf7L22Z+f9aaRv3tuTRmWlSrF0ucagiQF54exBTlJCSwMMZzK/g91JpLpglQpB1rzmeaT3slhC5WTbddeGjVofcnr6dqS5wuI85xPACbavqNCXIcOTkn5UdWQnScBk54KQ3ZI1RqIPHa5RJtGiqFx/iUvKZDW1DCEYHjEi6x9WJZVdaTEHeJntpyU9O0ibxsiZda1wVZPftNckvbauaHAFh6rCrO3v5XPAaiXbNkWtPtj+HfA71RBQLtB4eNLpG3KxNM7ewFBZh7X70ERYz/apuS9VaWoZQmOqLxOJcGT9w2+AqyT9Lu+/Czi+kCXWBN3CNzfmLMeTWXDhbBVlj3Zn9A8W1JUbFGcFhwb4NXCNpNNzX7Zn6A/ZkaCpPmyvJK1U+UOvF8KTDfdhWGG9Jr4g5fnniyUd3VqGUINGVF8mEuHI+gTb35J0NvPVwfeyfV1+etdu7VWufEm3BajLkyadi2HASpl23ZyRpHVs15FKKtOHAyX9nlSFF+Ajtm8cyWuOYB/qfOi9ipTksiwLSiw9AXxsGH0ZisHmMGr3oRDWW7EtKWppYHLXjjX3BQngKaWSNOuwYKmkKqHzKfn3O4HjbT+cIrXBYIQj6xMkHQqcaPvQ4bR3ifIltm/pYqNJhY/B+DXVqyRXxkl+qUr5mV7tw7A/9BoOT3ajY6ca6kOTYb3NgTWdFGNWAJZqn7vrwnEkqa9tSMlRHyatlazCaKm+jBvCkfUPNwD7SXolqSrxiYURWVmeAW5W0rWrXL4k/0N9jDSX8/y9UzF1vutlGrQ1EWjiQ2/Y4ckWDaxnG3YfmgrrqaAYQ1qAvChpZFtFMeYFtn8hae9Cv0rLwQHY/qqkg5mv+vIUqTp6q59vLZmlO2GIrMU+Q6lkyXuB9wGr2l6zQtta5UskXQFcSlro+ryGXZ4gb4ShMieDzqim1JVyKZ8cntwO+BxwYZVMu05/N+WyJKPYhxWBLzPMsJ7S4v71gRs8v1J2pcrMylWxc5bxYaSlJafYXqOsjRLXiP+RNmJE1n+8AliLNCqqlORh+xilgoWr2r5zGNdewnapxIxgVHk1sJqk4v/zryq0H3Z4ssFkjSbmheqG9eoqxgB8O6+z/AIp0WRpUkp+k0TUoo3QWuwTJB0s6W6SQvktwAa239WlWbuNdwEzSKEbJK2nalqLZ0l6Z5VrDoNKRTYnOpJ+DRxCSgLaMG/Th2y0MK3w5HTggorhyfZkjdb2eqoljNTpQ4sX2P4FMMf2xTnkvUmF9u2KMedTQjGmjR1Jka5bnGrFvZWUkdokEUZrI0KLfYKSsO2pwMuBxVrHbVdRg7ielHF4USF0crPLV1Z+AliC5GzmUH39FpIucFuNqU7HgnIoVape2zX/kRsIT9ZOGGmgD7XDekqKMW8j3dvnVp2L6rTousxC7IrXiNBiGxFa7B/mkcRmX0IaVW1CKvpXJa13ru3H2kI2VT4AlyGl+q9u+wBJq5IU9bsiaSrJCa6QP7BanViapPIdDI9bgBcBD9S0Uzc8WTthpIE+1ArrSfocSV2lTiLFJEnL2X4k21yeip+zkhaz/ewQx+6v0b9xSTiy/uGzpLDRVba3znMTVdXsb5H0fmCypDWzzdJSRsCPgQGS8zyAtNbnVObXbxqKj5M+VFYmJYu0HNnj2W4wPFYAbpN0DfD8h58rVDPO4ck1SF+QWkk8ppoTqbWIt6E+7AhclpeRbJ2dyCHM1yftxtLAuZIeJhXnPMX2PytcH1JlhSsknULq/07AgRVtXMnCS1CeP2b7PQu1mOCEI+sfnrH9jKTWt7M7JL2qoo3PkEpVPEsSRj0X+FaF9hvbfr2kGwFsPyJp0TIN8/q3QyV9xvYPK/Y7GJz9G7AxnfrhybrJGk30YV3bj7Z2ch9Kh/Rsf5NUmmhdkmDwxZJmdRERaLfxK0nXkb7sCXiP7VJJWZJeRKo/tnjudzFqsUTZPkxEwpH1D7MkLQv8FviDpEeorhr/FLBvXqNidy5gOBRzlBRCWlldK5JGaFX4h6SlbD8haT/St8xv58XBQUXcpTRPSZoIT9Zdz9ZEH2qH9TIPkqTT/g2sVLVxdlzDkY37D2A30vRBsaTQE8B/DcPehCGSPfoQSW8kzVedY7t0lp+kDUlVnlvqB48Bu9u+vmT7XUnfVF9P0gXcAdjPdpkaSy0bM22vq6Sg8B1S6Oe/bG9c1kYAki6zvbkW1gkcTgLOhcB6pOrIwwpPZjvDTtZoog+SPgR8jSQ2/HxYz/avS7b/BOn+XjHbOLHsaKpJJL23ybWZE4FwZBMISTOBT9m+NO9vDvyk4oLPtYA3kz4wL7B9e8U+3Gh7fUnfAW62/Zums7qCauQvRgtRdbQn6Q0srPpSao6rwT6szfyw3gVVHJGkg0glcGZUuWZTSPqA7WMlfYHOxWdHo/BrXxKhxYnFEy0nBmD7svyNvjS27wDuqNGHv+W1Om8BDpa0GLGecUxpIjxZN1mjoRBpnbBeSxpqc0kfcdJaXBGY5mpai3VoLcCeNuRZwULEiGwCIKmVAfVB0qTx8aQPmZ2BR2zvO4p9WQJ4O2k0drekFwOvtX3eaPUhSDQcnhzWerYm+1AXFbQWbb9S0sqkdPwqWosjjqSv2f7OWPejlwhHNgHI8w+DYY9ideYWklZiQT28v4x2H4LmkHQy8FnbddezjRlqQGtxNIgF0QsTocUJQJbK6QkkvZu01mZlUnbYqqRQ5Tpj2a+gNrXXs/UATWgtjgahtdhGOLIJRE7f/xALT8iXKuPSEN8iqZKcn5M+tgZ2GcXrByPD/mPdgQZo11rcnepai6NBhNHaCEc2sTgbuAq4merrv5piju1/S5okaZLtC/O6tqCPaSpZYyyxfUjWWnycJIb89ZpyVSNFjMjaCEc2sZhq+/PdTxtRHpU0DbgEOE7Sg8DcMe5TMEx6KVmjCbLjGlPnJWkz25cPcaz0us2JQiR7TCCyKOps4CwWnMd4eBT7sCRJ8UEkAeJlgONs/3u0+hAERTo44QUYbWfcKZkjEjyGJkZkE4vngO+R9BZb/7gmlYYZFWw/WdgtVZk6CEYS20sBSDqAJE31a+Z/0VpqiKaNImlT4A3AipKKkZOlgcmj1Y9+JBzZxOLzwCtsPzTaFx7iW29fhqCCccl/tEml/VTS1cB3R+n6i5IWQy/Cgg70cZIcXDAI4cgmFrcCT43FhVvfeoOgh5mX9URPIH3p2oX5KiUjTk6YuVjS0bb/PFrXHQ+EI5tYzANm5AXSxTmy0Uy/D4Je5f3AoXkzcHk+Nto8Jel7pLWVRdGAURcu6BfCkU0sfpu3IAjasH0/sO1gz4+iNNRxwInANsBewIeBf43CdfuWyFoMgiAowWhlDkq63vYGRXksSRfb7lghIIgR2YRC0n10Lg8xalmLQdDHjNZC5Dn59wOS/pNUQPclo3TtviQc2cRieuHxVGBHYPkx6ksQ9BujFb76tqRlgC8APySl3+8zStfuS6IO1ATC9r8L299s/4BUhDAIgu6M1ohsR9K0zy1Z8PutwPajdO2+JEZkE4hCXTJIX2KmM4oLPoOgl+khaah1bT/a2rH9sKSooD4Ekewxgchp960/+FzgfuAQ23eNWaeCoEfoFWkoSTcBW9l+JO8vD1xs+7Wj2Y9+IkZkE4t3AO9lwTIu7wMOGKsOBcFY04PSUP8LXCHpFNIXz52AA8egH31DOLKJxW+BR4EbSMK9QRD0mDSU7V9Juo40fy3gPbZvG+1+9BMRWpxASLrF9mvGuh9B0ItIellIQ/UnMSKbWFwh6bW2bx7rjgRBDxLSUH1KpN9PLDYHrpd0p6SZkm6WNHOsOxUEPcJxwB3A6sA3SclQ145lh4JyRGhxAiHpZZ2ORzglCEIaqp+J0OIEIhxWEAxJSEP1KeHIgiAIEiEN1afEHFkQBEEipKH6lHBkQRAEiYWkoYCQhuoDwpEFQRAkJklarrWTpaFi+qUPiD9SEARBIqSh+pRIvw+CIMhIWpv50lAXhDRUfxCOLAiCIOhrYo4sCIIg6GvCkQVBEAR9TTiyIAiCoK8JRxYEQRD0Nf8fEW/XEy6pJ7AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>number_ratings</th>\n",
       "      <th>enrollment</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>lectures</th>\n",
       "      <th>duration</th>\n",
       "      <th>price2</th>\n",
       "      <th>discount</th>\n",
       "      <th>inst_rating</th>\n",
       "      <th>inst_review</th>\n",
       "      <th>inst_student</th>\n",
       "      <th>inst_course</th>\n",
       "      <th>cat_design</th>\n",
       "      <th>cat_development</th>\n",
       "      <th>cat_hobby</th>\n",
       "      <th>cat_it_software</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>avg_rating</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200454</td>\n",
       "      <td>0.207510</td>\n",
       "      <td>-0.013175</td>\n",
       "      <td>0.019096</td>\n",
       "      <td>-0.017504</td>\n",
       "      <td>-0.082052</td>\n",
       "      <td>-0.001695</td>\n",
       "      <td>0.736497</td>\n",
       "      <td>0.188549</td>\n",
       "      <td>0.127469</td>\n",
       "      <td>-0.062877</td>\n",
       "      <td>0.065560</td>\n",
       "      <td>0.010766</td>\n",
       "      <td>0.161467</td>\n",
       "      <td>-0.141447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_ratings</th>\n",
       "      <td>0.200454</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.853691</td>\n",
       "      <td>-0.011226</td>\n",
       "      <td>0.010322</td>\n",
       "      <td>0.026363</td>\n",
       "      <td>-0.035223</td>\n",
       "      <td>-0.011101</td>\n",
       "      <td>0.242893</td>\n",
       "      <td>0.346876</td>\n",
       "      <td>0.308930</td>\n",
       "      <td>-0.036057</td>\n",
       "      <td>-0.025443</td>\n",
       "      <td>0.076669</td>\n",
       "      <td>-0.048992</td>\n",
       "      <td>-0.069379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enrollment</th>\n",
       "      <td>0.207510</td>\n",
       "      <td>0.853691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.039707</td>\n",
       "      <td>0.007326</td>\n",
       "      <td>0.019542</td>\n",
       "      <td>-0.049769</td>\n",
       "      <td>0.003227</td>\n",
       "      <td>0.237112</td>\n",
       "      <td>0.380016</td>\n",
       "      <td>0.421044</td>\n",
       "      <td>-0.024146</td>\n",
       "      <td>-0.006974</td>\n",
       "      <td>0.064016</td>\n",
       "      <td>-0.020714</td>\n",
       "      <td>-0.096815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_updated</th>\n",
       "      <td>-0.013175</td>\n",
       "      <td>-0.011226</td>\n",
       "      <td>-0.039707</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077893</td>\n",
       "      <td>0.085966</td>\n",
       "      <td>-0.010121</td>\n",
       "      <td>0.068650</td>\n",
       "      <td>0.030413</td>\n",
       "      <td>0.065714</td>\n",
       "      <td>0.042089</td>\n",
       "      <td>-0.035199</td>\n",
       "      <td>0.029848</td>\n",
       "      <td>0.118703</td>\n",
       "      <td>-0.117070</td>\n",
       "      <td>0.063106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lectures</th>\n",
       "      <td>0.019096</td>\n",
       "      <td>0.010322</td>\n",
       "      <td>0.007326</td>\n",
       "      <td>0.077893</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.717903</td>\n",
       "      <td>0.022148</td>\n",
       "      <td>-0.029648</td>\n",
       "      <td>0.029788</td>\n",
       "      <td>-0.012769</td>\n",
       "      <td>-0.001241</td>\n",
       "      <td>-0.006728</td>\n",
       "      <td>-0.032364</td>\n",
       "      <td>0.206430</td>\n",
       "      <td>-0.074052</td>\n",
       "      <td>-0.015455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <td>-0.017504</td>\n",
       "      <td>0.026363</td>\n",
       "      <td>0.019542</td>\n",
       "      <td>0.085966</td>\n",
       "      <td>0.717903</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>-0.002138</td>\n",
       "      <td>0.013956</td>\n",
       "      <td>-0.002242</td>\n",
       "      <td>0.017686</td>\n",
       "      <td>-0.002731</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.201118</td>\n",
       "      <td>-0.089220</td>\n",
       "      <td>-0.008402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price2</th>\n",
       "      <td>-0.082052</td>\n",
       "      <td>-0.035223</td>\n",
       "      <td>-0.049769</td>\n",
       "      <td>-0.010121</td>\n",
       "      <td>0.022148</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027805</td>\n",
       "      <td>-0.051260</td>\n",
       "      <td>-0.024331</td>\n",
       "      <td>-0.011644</td>\n",
       "      <td>-0.000523</td>\n",
       "      <td>-0.020825</td>\n",
       "      <td>-0.074453</td>\n",
       "      <td>0.015422</td>\n",
       "      <td>0.008970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discount</th>\n",
       "      <td>-0.001695</td>\n",
       "      <td>-0.011101</td>\n",
       "      <td>0.003227</td>\n",
       "      <td>0.068650</td>\n",
       "      <td>-0.029648</td>\n",
       "      <td>-0.002138</td>\n",
       "      <td>0.027805</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006355</td>\n",
       "      <td>-0.036782</td>\n",
       "      <td>-0.045179</td>\n",
       "      <td>0.014594</td>\n",
       "      <td>-0.008311</td>\n",
       "      <td>-0.025417</td>\n",
       "      <td>-0.017846</td>\n",
       "      <td>-0.057408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inst_rating</th>\n",
       "      <td>0.736497</td>\n",
       "      <td>0.242893</td>\n",
       "      <td>0.237112</td>\n",
       "      <td>0.030413</td>\n",
       "      <td>0.029788</td>\n",
       "      <td>0.013956</td>\n",
       "      <td>-0.051260</td>\n",
       "      <td>-0.006355</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.288648</td>\n",
       "      <td>0.188606</td>\n",
       "      <td>-0.130596</td>\n",
       "      <td>0.071306</td>\n",
       "      <td>0.019454</td>\n",
       "      <td>0.142371</td>\n",
       "      <td>-0.127695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inst_review</th>\n",
       "      <td>0.188549</td>\n",
       "      <td>0.346876</td>\n",
       "      <td>0.380016</td>\n",
       "      <td>0.065714</td>\n",
       "      <td>-0.012769</td>\n",
       "      <td>-0.002242</td>\n",
       "      <td>-0.024331</td>\n",
       "      <td>-0.036782</td>\n",
       "      <td>0.288648</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871341</td>\n",
       "      <td>0.176409</td>\n",
       "      <td>-0.022900</td>\n",
       "      <td>0.079639</td>\n",
       "      <td>-0.098813</td>\n",
       "      <td>0.035249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inst_student</th>\n",
       "      <td>0.127469</td>\n",
       "      <td>0.308930</td>\n",
       "      <td>0.421044</td>\n",
       "      <td>0.042089</td>\n",
       "      <td>-0.001241</td>\n",
       "      <td>0.017686</td>\n",
       "      <td>-0.011644</td>\n",
       "      <td>-0.045179</td>\n",
       "      <td>0.188606</td>\n",
       "      <td>0.871341</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.242852</td>\n",
       "      <td>-0.026477</td>\n",
       "      <td>0.126037</td>\n",
       "      <td>-0.099872</td>\n",
       "      <td>0.007515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inst_course</th>\n",
       "      <td>-0.062877</td>\n",
       "      <td>-0.036057</td>\n",
       "      <td>-0.024146</td>\n",
       "      <td>-0.035199</td>\n",
       "      <td>-0.006728</td>\n",
       "      <td>-0.002731</td>\n",
       "      <td>-0.000523</td>\n",
       "      <td>0.014594</td>\n",
       "      <td>-0.130596</td>\n",
       "      <td>0.176409</td>\n",
       "      <td>0.242852</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.036259</td>\n",
       "      <td>0.061448</td>\n",
       "      <td>-0.018124</td>\n",
       "      <td>-0.009558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_design</th>\n",
       "      <td>0.065560</td>\n",
       "      <td>-0.025443</td>\n",
       "      <td>-0.006974</td>\n",
       "      <td>0.029848</td>\n",
       "      <td>-0.032364</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>-0.020825</td>\n",
       "      <td>-0.008311</td>\n",
       "      <td>0.071306</td>\n",
       "      <td>-0.022900</td>\n",
       "      <td>-0.026477</td>\n",
       "      <td>-0.036259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.169867</td>\n",
       "      <td>-0.090258</td>\n",
       "      <td>-0.164233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_development</th>\n",
       "      <td>0.010766</td>\n",
       "      <td>0.076669</td>\n",
       "      <td>0.064016</td>\n",
       "      <td>0.118703</td>\n",
       "      <td>0.206430</td>\n",
       "      <td>0.201118</td>\n",
       "      <td>-0.074453</td>\n",
       "      <td>-0.025417</td>\n",
       "      <td>0.019454</td>\n",
       "      <td>0.079639</td>\n",
       "      <td>0.126037</td>\n",
       "      <td>0.061448</td>\n",
       "      <td>-0.169867</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.175334</td>\n",
       "      <td>-0.319036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_hobby</th>\n",
       "      <td>0.161467</td>\n",
       "      <td>-0.048992</td>\n",
       "      <td>-0.020714</td>\n",
       "      <td>-0.117070</td>\n",
       "      <td>-0.074052</td>\n",
       "      <td>-0.089220</td>\n",
       "      <td>0.015422</td>\n",
       "      <td>-0.017846</td>\n",
       "      <td>0.142371</td>\n",
       "      <td>-0.098813</td>\n",
       "      <td>-0.099872</td>\n",
       "      <td>-0.018124</td>\n",
       "      <td>-0.090258</td>\n",
       "      <td>-0.175334</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.169518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_it_software</th>\n",
       "      <td>-0.141447</td>\n",
       "      <td>-0.069379</td>\n",
       "      <td>-0.096815</td>\n",
       "      <td>0.063106</td>\n",
       "      <td>-0.015455</td>\n",
       "      <td>-0.008402</td>\n",
       "      <td>0.008970</td>\n",
       "      <td>-0.057408</td>\n",
       "      <td>-0.127695</td>\n",
       "      <td>0.035249</td>\n",
       "      <td>0.007515</td>\n",
       "      <td>-0.009558</td>\n",
       "      <td>-0.164233</td>\n",
       "      <td>-0.319036</td>\n",
       "      <td>-0.169518</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 avg_rating  number_ratings  enrollment  last_updated  \\\n",
       "avg_rating         1.000000        0.200454    0.207510     -0.013175   \n",
       "number_ratings     0.200454        1.000000    0.853691     -0.011226   \n",
       "enrollment         0.207510        0.853691    1.000000     -0.039707   \n",
       "last_updated      -0.013175       -0.011226   -0.039707      1.000000   \n",
       "lectures           0.019096        0.010322    0.007326      0.077893   \n",
       "duration          -0.017504        0.026363    0.019542      0.085966   \n",
       "price2            -0.082052       -0.035223   -0.049769     -0.010121   \n",
       "discount          -0.001695       -0.011101    0.003227      0.068650   \n",
       "inst_rating        0.736497        0.242893    0.237112      0.030413   \n",
       "inst_review        0.188549        0.346876    0.380016      0.065714   \n",
       "inst_student       0.127469        0.308930    0.421044      0.042089   \n",
       "inst_course       -0.062877       -0.036057   -0.024146     -0.035199   \n",
       "cat_design         0.065560       -0.025443   -0.006974      0.029848   \n",
       "cat_development    0.010766        0.076669    0.064016      0.118703   \n",
       "cat_hobby          0.161467       -0.048992   -0.020714     -0.117070   \n",
       "cat_it_software   -0.141447       -0.069379   -0.096815      0.063106   \n",
       "\n",
       "                 lectures  duration    price2  discount  inst_rating  \\\n",
       "avg_rating       0.019096 -0.017504 -0.082052 -0.001695     0.736497   \n",
       "number_ratings   0.010322  0.026363 -0.035223 -0.011101     0.242893   \n",
       "enrollment       0.007326  0.019542 -0.049769  0.003227     0.237112   \n",
       "last_updated     0.077893  0.085966 -0.010121  0.068650     0.030413   \n",
       "lectures         1.000000  0.717903  0.022148 -0.029648     0.029788   \n",
       "duration         0.717903  1.000000  0.001551 -0.002138     0.013956   \n",
       "price2           0.022148  0.001551  1.000000  0.027805    -0.051260   \n",
       "discount        -0.029648 -0.002138  0.027805  1.000000    -0.006355   \n",
       "inst_rating      0.029788  0.013956 -0.051260 -0.006355     1.000000   \n",
       "inst_review     -0.012769 -0.002242 -0.024331 -0.036782     0.288648   \n",
       "inst_student    -0.001241  0.017686 -0.011644 -0.045179     0.188606   \n",
       "inst_course     -0.006728 -0.002731 -0.000523  0.014594    -0.130596   \n",
       "cat_design      -0.032364  0.004014 -0.020825 -0.008311     0.071306   \n",
       "cat_development  0.206430  0.201118 -0.074453 -0.025417     0.019454   \n",
       "cat_hobby       -0.074052 -0.089220  0.015422 -0.017846     0.142371   \n",
       "cat_it_software -0.015455 -0.008402  0.008970 -0.057408    -0.127695   \n",
       "\n",
       "                 inst_review  inst_student  inst_course  cat_design  \\\n",
       "avg_rating          0.188549      0.127469    -0.062877    0.065560   \n",
       "number_ratings      0.346876      0.308930    -0.036057   -0.025443   \n",
       "enrollment          0.380016      0.421044    -0.024146   -0.006974   \n",
       "last_updated        0.065714      0.042089    -0.035199    0.029848   \n",
       "lectures           -0.012769     -0.001241    -0.006728   -0.032364   \n",
       "duration           -0.002242      0.017686    -0.002731    0.004014   \n",
       "price2             -0.024331     -0.011644    -0.000523   -0.020825   \n",
       "discount           -0.036782     -0.045179     0.014594   -0.008311   \n",
       "inst_rating         0.288648      0.188606    -0.130596    0.071306   \n",
       "inst_review         1.000000      0.871341     0.176409   -0.022900   \n",
       "inst_student        0.871341      1.000000     0.242852   -0.026477   \n",
       "inst_course         0.176409      0.242852     1.000000   -0.036259   \n",
       "cat_design         -0.022900     -0.026477    -0.036259    1.000000   \n",
       "cat_development     0.079639      0.126037     0.061448   -0.169867   \n",
       "cat_hobby          -0.098813     -0.099872    -0.018124   -0.090258   \n",
       "cat_it_software     0.035249      0.007515    -0.009558   -0.164233   \n",
       "\n",
       "                 cat_development  cat_hobby  cat_it_software  \n",
       "avg_rating              0.010766   0.161467        -0.141447  \n",
       "number_ratings          0.076669  -0.048992        -0.069379  \n",
       "enrollment              0.064016  -0.020714        -0.096815  \n",
       "last_updated            0.118703  -0.117070         0.063106  \n",
       "lectures                0.206430  -0.074052        -0.015455  \n",
       "duration                0.201118  -0.089220        -0.008402  \n",
       "price2                 -0.074453   0.015422         0.008970  \n",
       "discount               -0.025417  -0.017846        -0.057408  \n",
       "inst_rating             0.019454   0.142371        -0.127695  \n",
       "inst_review             0.079639  -0.098813         0.035249  \n",
       "inst_student            0.126037  -0.099872         0.007515  \n",
       "inst_course             0.061448  -0.018124        -0.009558  \n",
       "cat_design             -0.169867  -0.090258        -0.164233  \n",
       "cat_development         1.000000  -0.175334        -0.319036  \n",
       "cat_hobby              -0.175334   1.000000        -0.169518  \n",
       "cat_it_software        -0.319036  -0.169518         1.000000  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## checking the correlation\n",
    "df.corr()\n",
    "\n",
    "# plotting correlation heatmap\n",
    "dataplot=sns.heatmap(df.corr())\n",
    "\n",
    "# displaying heatmap\n",
    "plt.show()\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0596de86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrollment         1.000000\n",
      "number_ratings     0.853691\n",
      "inst_student       0.421044\n",
      "inst_review        0.380016\n",
      "inst_rating        0.237112\n",
      "avg_rating         0.207510\n",
      "cat_it_software    0.096815\n",
      "cat_development    0.064016\n",
      "price2             0.049769\n",
      "last_updated       0.039707\n",
      "inst_course        0.024146\n",
      "cat_hobby          0.020714\n",
      "duration           0.019542\n",
      "lectures           0.007326\n",
      "cat_design         0.006974\n",
      "discount           0.003227\n",
      "Name: enrollment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# absolute value of the correlation \n",
    "absolute_Corr = df.corr().abs()\n",
    "# sorted value\n",
    "print(absolute_Corr[\"enrollment\"].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297f2608",
   "metadata": {},
   "source": [
    "## Feature Observation and Analysis\n",
    "From our previous observation, we can see that number_ratings has high correlation with enrollment. inst_student has moderate positive correlation with enrollment followed by inst_review.\n",
    "Moreover, inst_rating and avg_rating has high positive correlation with each other. It is better to consider one of them for analysis. Duration and lectures also has positive high correlation. Inst_review and inst_student also has high positive correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1400ba",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e4b002ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='enrollment', ylabel='Density'>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution and histogram plot of the enrollment\n",
    "from scipy.stats import norm\n",
    "sns.distplot(df.enrollment, fit=norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f7089",
   "metadata": {},
   "source": [
    "We can see that the enrollment distribution (after we removed some outliers)still has a longer tail on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dec31803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='number_ratings', ylabel='Density'>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fitting plot for number of ratings\n",
    "sns.distplot(df.number_ratings , fit=norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaef55b",
   "metadata": {},
   "source": [
    "The number of ratings distribution does not follow normal distribution as it still skewed to the right. Most of the ratings lie between 0 to 1300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e25875f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='inst_review', ylabel='Density'>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fitting plot for inst_reviews\n",
    "sns.distplot(df.inst_review, fit=norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57883fa5",
   "metadata": {},
   "source": [
    "The inst_review distribution does not follow normal distribution with as it still skewed to the right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b2661c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='inst_student', ylabel='Density'>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fitting plot for inst_reviews\n",
    "sns.distplot(df.inst_student, fit=norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e3c569",
   "metadata": {},
   "source": [
    "The inst_student distribution does not follow normal distribution as it still skewed to the right but still we can consider it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f1acf",
   "metadata": {},
   "source": [
    "## 4) Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6649fcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>number_ratings</th>\n",
       "      <th>enrollment</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>lectures</th>\n",
       "      <th>duration</th>\n",
       "      <th>price2</th>\n",
       "      <th>discount</th>\n",
       "      <th>inst_rating</th>\n",
       "      <th>inst_review</th>\n",
       "      <th>inst_student</th>\n",
       "      <th>inst_course</th>\n",
       "      <th>cat_design</th>\n",
       "      <th>cat_development</th>\n",
       "      <th>cat_hobby</th>\n",
       "      <th>cat_it_software</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>6833.0</td>\n",
       "      <td>21218.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.23</td>\n",
       "      <td>23.99</td>\n",
       "      <td>91.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>19993.0</td>\n",
       "      <td>55621.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.5</td>\n",
       "      <td>2826.0</td>\n",
       "      <td>14269.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>41.0</td>\n",
       "      <td>7.13</td>\n",
       "      <td>19.99</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>11922.0</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.4</td>\n",
       "      <td>991.0</td>\n",
       "      <td>3563.0</td>\n",
       "      <td>20.67</td>\n",
       "      <td>91.0</td>\n",
       "      <td>5.23</td>\n",
       "      <td>28.99</td>\n",
       "      <td>77.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2193.0</td>\n",
       "      <td>7422.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.2</td>\n",
       "      <td>681.0</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>57.0</td>\n",
       "      <td>5.05</td>\n",
       "      <td>16.99</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>25989.0</td>\n",
       "      <td>242683.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.3</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>14048.0</td>\n",
       "      <td>13.33</td>\n",
       "      <td>142.0</td>\n",
       "      <td>10.77</td>\n",
       "      <td>20.99</td>\n",
       "      <td>55.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>25581.0</td>\n",
       "      <td>113480.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>4.4</td>\n",
       "      <td>906.0</td>\n",
       "      <td>5293.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>43.0</td>\n",
       "      <td>4.75</td>\n",
       "      <td>17.99</td>\n",
       "      <td>86.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2291.0</td>\n",
       "      <td>16633.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>4.4</td>\n",
       "      <td>345.0</td>\n",
       "      <td>6212.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.90</td>\n",
       "      <td>16.99</td>\n",
       "      <td>89.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>9250.0</td>\n",
       "      <td>159228.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>3.8</td>\n",
       "      <td>693.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>20.75</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.13</td>\n",
       "      <td>21.99</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>693.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>4.2</td>\n",
       "      <td>3459.0</td>\n",
       "      <td>13987.0</td>\n",
       "      <td>15.50</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.58</td>\n",
       "      <td>16.99</td>\n",
       "      <td>81.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>9123.0</td>\n",
       "      <td>64043.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>4.5</td>\n",
       "      <td>4546.0</td>\n",
       "      <td>15793.0</td>\n",
       "      <td>20.50</td>\n",
       "      <td>267.0</td>\n",
       "      <td>32.73</td>\n",
       "      <td>69.99</td>\n",
       "      <td>51.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>39566.0</td>\n",
       "      <td>149857.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      avg_rating  number_ratings  enrollment  last_updated  lectures  \\\n",
       "0            4.6          6833.0     21218.0         20.83      53.0   \n",
       "1            4.5          2826.0     14269.0         20.83      41.0   \n",
       "2            4.4           991.0      3563.0         20.67      91.0   \n",
       "3            4.2           681.0      1932.0         20.83      57.0   \n",
       "4            4.3          1939.0     14048.0         13.33     142.0   \n",
       "...          ...             ...         ...           ...       ...   \n",
       "1450         4.4           906.0      5293.0         20.83      43.0   \n",
       "1451         4.4           345.0      6212.0         20.83      10.0   \n",
       "1452         3.8           693.0      3254.0         20.75      20.0   \n",
       "1453         4.2          3459.0     13987.0         15.50      30.0   \n",
       "1454         4.5          4546.0     15793.0         20.50     267.0   \n",
       "\n",
       "      duration  price2  discount  inst_rating  inst_review  inst_student  \\\n",
       "0         5.23   23.99      91.0          4.6      19993.0       55621.0   \n",
       "1         7.13   19.99      88.0          4.5      11922.0       53659.0   \n",
       "2         5.23   28.99      77.0          4.4       2193.0        7422.0   \n",
       "3         5.05   16.99      64.0          4.3      25989.0      242683.0   \n",
       "4        10.77   20.99      55.0          4.5      25581.0      113480.0   \n",
       "...        ...     ...       ...          ...          ...           ...   \n",
       "1450      4.75   17.99      86.0          4.4       2291.0       16633.0   \n",
       "1451      5.90   16.99      89.0          4.2       9250.0      159228.0   \n",
       "1452      3.13   21.99      90.0          3.8        693.0        3254.0   \n",
       "1453      1.58   16.99      81.0          4.2       9123.0       64043.0   \n",
       "1454     32.73   69.99      51.0          4.4      39566.0      149857.0   \n",
       "\n",
       "      inst_course  cat_design  cat_development  cat_hobby  cat_it_software  \n",
       "0             3.0           0                1          0                0  \n",
       "1            10.0           0                0          0                0  \n",
       "2             6.0           0                0          0                0  \n",
       "3            43.0           0                0          0                1  \n",
       "4            12.0           0                0          0                0  \n",
       "...           ...         ...              ...        ...              ...  \n",
       "1450         10.0           0                0          0                0  \n",
       "1451         19.0           0                0          0                1  \n",
       "1452          1.0           0                0          0                0  \n",
       "1453         26.0           0                0          0                0  \n",
       "1454        226.0           0                0          0                1  \n",
       "\n",
       "[1455 rows x 16 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## copy of original dataframe\n",
    "df1 = df.copy()\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e9400",
   "metadata": {},
   "source": [
    "### 4.1) Assigning the enrollment column of the dataframe to a new variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "355ddf77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = df1.enrollment\n",
    "## creating another variabzle to save the features except target column\n",
    "features = df1.drop('enrollment', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf6fbdc",
   "metadata": {},
   "source": [
    "### 4.2) Manual Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b5a4286b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrollment         1.000000\n",
      "number_ratings     0.853691\n",
      "inst_student       0.421044\n",
      "inst_review        0.380016\n",
      "inst_rating        0.237112\n",
      "avg_rating         0.207510\n",
      "cat_it_software    0.096815\n",
      "cat_development    0.064016\n",
      "price2             0.049769\n",
      "last_updated       0.039707\n",
      "inst_course        0.024146\n",
      "cat_hobby          0.020714\n",
      "duration           0.019542\n",
      "lectures           0.007326\n",
      "cat_design         0.006974\n",
      "discount           0.003227\n",
      "Name: enrollment, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_ratings</th>\n",
       "      <th>inst_student</th>\n",
       "      <th>inst_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6833.0</td>\n",
       "      <td>55621.0</td>\n",
       "      <td>19993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2826.0</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>11922.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>991.0</td>\n",
       "      <td>7422.0</td>\n",
       "      <td>2193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>681.0</td>\n",
       "      <td>242683.0</td>\n",
       "      <td>25989.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1939.0</td>\n",
       "      <td>113480.0</td>\n",
       "      <td>25581.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>906.0</td>\n",
       "      <td>16633.0</td>\n",
       "      <td>2291.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>345.0</td>\n",
       "      <td>159228.0</td>\n",
       "      <td>9250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>693.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>693.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>3459.0</td>\n",
       "      <td>64043.0</td>\n",
       "      <td>9123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>4546.0</td>\n",
       "      <td>149857.0</td>\n",
       "      <td>39566.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      number_ratings  inst_student  inst_review\n",
       "0             6833.0       55621.0      19993.0\n",
       "1             2826.0       53659.0      11922.0\n",
       "2              991.0        7422.0       2193.0\n",
       "3              681.0      242683.0      25989.0\n",
       "4             1939.0      113480.0      25581.0\n",
       "...              ...           ...          ...\n",
       "1450           906.0       16633.0       2291.0\n",
       "1451           345.0      159228.0       9250.0\n",
       "1452           693.0        3254.0        693.0\n",
       "1453          3459.0       64043.0       9123.0\n",
       "1454          4546.0      149857.0      39566.0\n",
       "\n",
       "[1455 rows x 3 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing correlation between all features\n",
    "df1.corr()\n",
    "# absolute value of the correlation \n",
    "absolute_Corr = df1.corr().abs()\n",
    "# sorted value\n",
    "print(absolute_Corr[\"enrollment\"].sort_values(ascending=False))\n",
    "\n",
    "## saving selected features in new dataframe\n",
    "df1_manual = pd.DataFrame(df1[['number_ratings','inst_student','inst_review']])\n",
    "df1_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bfc88e",
   "metadata": {},
   "source": [
    "### 4.3) Variance Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ccab275e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_ratings</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>lectures</th>\n",
       "      <th>duration</th>\n",
       "      <th>price2</th>\n",
       "      <th>discount</th>\n",
       "      <th>inst_review</th>\n",
       "      <th>inst_student</th>\n",
       "      <th>inst_course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6833.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.23</td>\n",
       "      <td>23.99</td>\n",
       "      <td>91.0</td>\n",
       "      <td>19993.0</td>\n",
       "      <td>55621.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2826.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>41.0</td>\n",
       "      <td>7.13</td>\n",
       "      <td>19.99</td>\n",
       "      <td>88.0</td>\n",
       "      <td>11922.0</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>991.0</td>\n",
       "      <td>20.67</td>\n",
       "      <td>91.0</td>\n",
       "      <td>5.23</td>\n",
       "      <td>28.99</td>\n",
       "      <td>77.0</td>\n",
       "      <td>2193.0</td>\n",
       "      <td>7422.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>681.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>57.0</td>\n",
       "      <td>5.05</td>\n",
       "      <td>16.99</td>\n",
       "      <td>64.0</td>\n",
       "      <td>25989.0</td>\n",
       "      <td>242683.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1939.0</td>\n",
       "      <td>13.33</td>\n",
       "      <td>142.0</td>\n",
       "      <td>10.77</td>\n",
       "      <td>20.99</td>\n",
       "      <td>55.0</td>\n",
       "      <td>25581.0</td>\n",
       "      <td>113480.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>906.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>43.0</td>\n",
       "      <td>4.75</td>\n",
       "      <td>17.99</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2291.0</td>\n",
       "      <td>16633.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>345.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.90</td>\n",
       "      <td>16.99</td>\n",
       "      <td>89.0</td>\n",
       "      <td>9250.0</td>\n",
       "      <td>159228.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>693.0</td>\n",
       "      <td>20.75</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.13</td>\n",
       "      <td>21.99</td>\n",
       "      <td>90.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>3459.0</td>\n",
       "      <td>15.50</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.58</td>\n",
       "      <td>16.99</td>\n",
       "      <td>81.0</td>\n",
       "      <td>9123.0</td>\n",
       "      <td>64043.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>4546.0</td>\n",
       "      <td>20.50</td>\n",
       "      <td>267.0</td>\n",
       "      <td>32.73</td>\n",
       "      <td>69.99</td>\n",
       "      <td>51.0</td>\n",
       "      <td>39566.0</td>\n",
       "      <td>149857.0</td>\n",
       "      <td>226.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      number_ratings  last_updated  lectures  duration  price2  discount  \\\n",
       "0             6833.0         20.83      53.0      5.23   23.99      91.0   \n",
       "1             2826.0         20.83      41.0      7.13   19.99      88.0   \n",
       "2              991.0         20.67      91.0      5.23   28.99      77.0   \n",
       "3              681.0         20.83      57.0      5.05   16.99      64.0   \n",
       "4             1939.0         13.33     142.0     10.77   20.99      55.0   \n",
       "...              ...           ...       ...       ...     ...       ...   \n",
       "1450           906.0         20.83      43.0      4.75   17.99      86.0   \n",
       "1451           345.0         20.83      10.0      5.90   16.99      89.0   \n",
       "1452           693.0         20.75      20.0      3.13   21.99      90.0   \n",
       "1453          3459.0         15.50      30.0      1.58   16.99      81.0   \n",
       "1454          4546.0         20.50     267.0     32.73   69.99      51.0   \n",
       "\n",
       "      inst_review  inst_student  inst_course  \n",
       "0         19993.0       55621.0          3.0  \n",
       "1         11922.0       53659.0         10.0  \n",
       "2          2193.0        7422.0          6.0  \n",
       "3         25989.0      242683.0         43.0  \n",
       "4         25581.0      113480.0         12.0  \n",
       "...           ...           ...          ...  \n",
       "1450       2291.0       16633.0         10.0  \n",
       "1451       9250.0      159228.0         19.0  \n",
       "1452        693.0        3254.0          1.0  \n",
       "1453       9123.0       64043.0         26.0  \n",
       "1454      39566.0      149857.0        226.0  \n",
       "\n",
       "[1455 rows x 9 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## selecting all numerical features\n",
    "df2 = features.select_dtypes(include = np.number)\n",
    "## threshold is used as 0.2\n",
    "vt = VarianceThreshold(threshold = 0.2)\n",
    "vt.fit(df2)\n",
    "## selected features are saved in new dataframe as df_vt\n",
    "pd.DataFrame({'Variance': vt.variances_, 'select_status': vt.get_support()}, index = df2.columns)\n",
    "df_vt = df2.iloc[:, vt.get_support()]\n",
    "df_vt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a04ba7",
   "metadata": {},
   "source": [
    "### 4.4) Select k-Best method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fd5368b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_ratings</th>\n",
       "      <th>inst_review</th>\n",
       "      <th>inst_student</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6833.0</td>\n",
       "      <td>19993.0</td>\n",
       "      <td>55621.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2826.0</td>\n",
       "      <td>11922.0</td>\n",
       "      <td>53659.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>991.0</td>\n",
       "      <td>2193.0</td>\n",
       "      <td>7422.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>681.0</td>\n",
       "      <td>25989.0</td>\n",
       "      <td>242683.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1939.0</td>\n",
       "      <td>25581.0</td>\n",
       "      <td>113480.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_ratings  inst_review  inst_student\n",
       "0          6833.0      19993.0       55621.0\n",
       "1          2826.0      11922.0       53659.0\n",
       "2           991.0       2193.0        7422.0\n",
       "3           681.0      25989.0      242683.0\n",
       "4          1939.0      25581.0      113480.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 features are selected\n",
    "kbest = SelectKBest(score_func = f_regression, k = 3)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, target)\n",
    "\n",
    "kbest.fit_transform(X_train, Y_train)\n",
    "df_selKBest = features.iloc[:, kbest.get_support()]\n",
    "df_selKBest.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00643c8e",
   "metadata": {},
   "source": [
    "## 5) Linear Regression Models with Feature Selection, Feature transformation and Scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d766fc6d",
   "metadata": {},
   "source": [
    "### Here, 75:25 for training and testing and random_state=42 is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f3faf2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df1.enrollment\n",
    "features = df1.drop('enrollment', axis=1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features,target, test_size=0.25, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e86a50",
   "metadata": {},
   "source": [
    "### 5.0) Initiate some lists to store your experiments’ result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "51e517dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection = []\n",
    "feature_transformation = []\n",
    "feature_scaling = []\n",
    "r2_values = []\n",
    "rmse_values = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e0f99",
   "metadata": {},
   "source": [
    "### 5.1) Linear regression model with all features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ee6d1963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared=  0.7526\n",
      "RMSE=  4450.289645095032\n"
     ]
    }
   ],
   "source": [
    "# building the model with original feature set\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# predicting the test dataset with original feature set\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating R2 metrics scores\n",
    "r2 = model.score(X_test, Y_test).round(4)\n",
    "print(\"R squared= \", r2)\n",
    "\n",
    "# Calculating RMSE metrics scores\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE= \", mse**0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5dae2256",
   "metadata": {},
   "outputs": [],
   "source": [
    "## appending the following information to the lists\n",
    "feature_selection.append(\"All\")\n",
    "feature_transformation.append(\"None\")\n",
    "feature_scaling.append(\"None\")\n",
    "r2_values.append(r2)\n",
    "rmse_values.append(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e66643b",
   "metadata": {},
   "source": [
    "### 5.2) Linear Regression model with All features and Polynomial Features transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "de3a24c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>number_ratings</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>lectures</th>\n",
       "      <th>duration</th>\n",
       "      <th>price2</th>\n",
       "      <th>discount</th>\n",
       "      <th>inst_rating</th>\n",
       "      <th>inst_review</th>\n",
       "      <th>inst_student</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_design^2</th>\n",
       "      <th>cat_design cat_development</th>\n",
       "      <th>cat_design cat_hobby</th>\n",
       "      <th>cat_design cat_it_software</th>\n",
       "      <th>cat_development^2</th>\n",
       "      <th>cat_development cat_hobby</th>\n",
       "      <th>cat_development cat_it_software</th>\n",
       "      <th>cat_hobby^2</th>\n",
       "      <th>cat_hobby cat_it_software</th>\n",
       "      <th>cat_it_software^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>6833.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.23</td>\n",
       "      <td>23.99</td>\n",
       "      <td>91.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>19993.0</td>\n",
       "      <td>55621.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.5</td>\n",
       "      <td>2826.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>41.0</td>\n",
       "      <td>7.13</td>\n",
       "      <td>19.99</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>11922.0</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.4</td>\n",
       "      <td>991.0</td>\n",
       "      <td>20.67</td>\n",
       "      <td>91.0</td>\n",
       "      <td>5.23</td>\n",
       "      <td>28.99</td>\n",
       "      <td>77.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2193.0</td>\n",
       "      <td>7422.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.2</td>\n",
       "      <td>681.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>57.0</td>\n",
       "      <td>5.05</td>\n",
       "      <td>16.99</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>25989.0</td>\n",
       "      <td>242683.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.3</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>13.33</td>\n",
       "      <td>142.0</td>\n",
       "      <td>10.77</td>\n",
       "      <td>20.99</td>\n",
       "      <td>55.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>25581.0</td>\n",
       "      <td>113480.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>4.4</td>\n",
       "      <td>906.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>43.0</td>\n",
       "      <td>4.75</td>\n",
       "      <td>17.99</td>\n",
       "      <td>86.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2291.0</td>\n",
       "      <td>16633.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>4.4</td>\n",
       "      <td>345.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.90</td>\n",
       "      <td>16.99</td>\n",
       "      <td>89.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>9250.0</td>\n",
       "      <td>159228.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>3.8</td>\n",
       "      <td>693.0</td>\n",
       "      <td>20.75</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.13</td>\n",
       "      <td>21.99</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>693.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>4.2</td>\n",
       "      <td>3459.0</td>\n",
       "      <td>15.50</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.58</td>\n",
       "      <td>16.99</td>\n",
       "      <td>81.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>9123.0</td>\n",
       "      <td>64043.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>4.5</td>\n",
       "      <td>4546.0</td>\n",
       "      <td>20.50</td>\n",
       "      <td>267.0</td>\n",
       "      <td>32.73</td>\n",
       "      <td>69.99</td>\n",
       "      <td>51.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>39566.0</td>\n",
       "      <td>149857.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455 rows × 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      avg_rating  number_ratings  last_updated  lectures  duration  price2  \\\n",
       "0            4.6          6833.0         20.83      53.0      5.23   23.99   \n",
       "1            4.5          2826.0         20.83      41.0      7.13   19.99   \n",
       "2            4.4           991.0         20.67      91.0      5.23   28.99   \n",
       "3            4.2           681.0         20.83      57.0      5.05   16.99   \n",
       "4            4.3          1939.0         13.33     142.0     10.77   20.99   \n",
       "...          ...             ...           ...       ...       ...     ...   \n",
       "1450         4.4           906.0         20.83      43.0      4.75   17.99   \n",
       "1451         4.4           345.0         20.83      10.0      5.90   16.99   \n",
       "1452         3.8           693.0         20.75      20.0      3.13   21.99   \n",
       "1453         4.2          3459.0         15.50      30.0      1.58   16.99   \n",
       "1454         4.5          4546.0         20.50     267.0     32.73   69.99   \n",
       "\n",
       "      discount  inst_rating  inst_review  inst_student  ...  cat_design^2  \\\n",
       "0         91.0          4.6      19993.0       55621.0  ...           0.0   \n",
       "1         88.0          4.5      11922.0       53659.0  ...           0.0   \n",
       "2         77.0          4.4       2193.0        7422.0  ...           0.0   \n",
       "3         64.0          4.3      25989.0      242683.0  ...           0.0   \n",
       "4         55.0          4.5      25581.0      113480.0  ...           0.0   \n",
       "...        ...          ...          ...           ...  ...           ...   \n",
       "1450      86.0          4.4       2291.0       16633.0  ...           0.0   \n",
       "1451      89.0          4.2       9250.0      159228.0  ...           0.0   \n",
       "1452      90.0          3.8        693.0        3254.0  ...           0.0   \n",
       "1453      81.0          4.2       9123.0       64043.0  ...           0.0   \n",
       "1454      51.0          4.4      39566.0      149857.0  ...           0.0   \n",
       "\n",
       "      cat_design cat_development  cat_design cat_hobby  \\\n",
       "0                            0.0                   0.0   \n",
       "1                            0.0                   0.0   \n",
       "2                            0.0                   0.0   \n",
       "3                            0.0                   0.0   \n",
       "4                            0.0                   0.0   \n",
       "...                          ...                   ...   \n",
       "1450                         0.0                   0.0   \n",
       "1451                         0.0                   0.0   \n",
       "1452                         0.0                   0.0   \n",
       "1453                         0.0                   0.0   \n",
       "1454                         0.0                   0.0   \n",
       "\n",
       "      cat_design cat_it_software  cat_development^2  \\\n",
       "0                            0.0                1.0   \n",
       "1                            0.0                0.0   \n",
       "2                            0.0                0.0   \n",
       "3                            0.0                0.0   \n",
       "4                            0.0                0.0   \n",
       "...                          ...                ...   \n",
       "1450                         0.0                0.0   \n",
       "1451                         0.0                0.0   \n",
       "1452                         0.0                0.0   \n",
       "1453                         0.0                0.0   \n",
       "1454                         0.0                0.0   \n",
       "\n",
       "      cat_development cat_hobby  cat_development cat_it_software  cat_hobby^2  \\\n",
       "0                           0.0                              0.0          0.0   \n",
       "1                           0.0                              0.0          0.0   \n",
       "2                           0.0                              0.0          0.0   \n",
       "3                           0.0                              0.0          0.0   \n",
       "4                           0.0                              0.0          0.0   \n",
       "...                         ...                              ...          ...   \n",
       "1450                        0.0                              0.0          0.0   \n",
       "1451                        0.0                              0.0          0.0   \n",
       "1452                        0.0                              0.0          0.0   \n",
       "1453                        0.0                              0.0          0.0   \n",
       "1454                        0.0                              0.0          0.0   \n",
       "\n",
       "      cat_hobby cat_it_software  cat_it_software^2  \n",
       "0                           0.0                0.0  \n",
       "1                           0.0                0.0  \n",
       "2                           0.0                0.0  \n",
       "3                           0.0                1.0  \n",
       "4                           0.0                0.0  \n",
       "...                         ...                ...  \n",
       "1450                        0.0                0.0  \n",
       "1451                        0.0                1.0  \n",
       "1452                        0.0                0.0  \n",
       "1453                        0.0                0.0  \n",
       "1454                        0.0                1.0  \n",
       "\n",
       "[1455 rows x 135 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## polynomial degree of 2 is used\n",
    "pff = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "## pf is variable for the PolynomialFeatures class \n",
    "pf = pff.fit(features)\n",
    "\n",
    "## using get_feature_names for the column name\n",
    "poly= pd.DataFrame(pff.transform(features), columns = pf.get_feature_names(features.columns))\n",
    "poly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "01b7a63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df1.enrollment\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(poly,target, test_size=0.25, random_state=42) \n",
    "      \n",
    "# building the model using original features and Polynomial Features transformation\n",
    "modelPolynomial = LinearRegression()\n",
    "modelPolynomial.fit(X_train, Y_train)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9e1cd91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared=  0.5838\n",
      "RMSE=  5771.942315282742\n"
     ]
    }
   ],
   "source": [
    "# predicting the test dataset using original features and Polynomial Features transformation\n",
    "Y_pred = modelPolynomial.predict(X_test)\n",
    "\n",
    "# calculating R-squared metrics scores\n",
    "r2_poly = modelPolynomial.score(X_test, Y_test).round(4)\n",
    "print(\"R squared= \", r2_poly)\n",
    "\n",
    "# calculating RMSE metrics scores\n",
    "mse_poly = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE= \", mse_poly**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "065c0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the following information to the lists\n",
    "feature_selection.append(\"All\")\n",
    "feature_transformation.append(\"Poly 2\")\n",
    "feature_scaling.append(\"None\")\n",
    "r2_values.append(r2_poly)\n",
    "rmse_values.append(mse_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4ca300a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>number_ratings</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>lectures</th>\n",
       "      <th>duration</th>\n",
       "      <th>price2</th>\n",
       "      <th>discount</th>\n",
       "      <th>inst_rating</th>\n",
       "      <th>inst_review</th>\n",
       "      <th>inst_student</th>\n",
       "      <th>inst_course</th>\n",
       "      <th>cat_design</th>\n",
       "      <th>cat_development</th>\n",
       "      <th>cat_hobby</th>\n",
       "      <th>cat_it_software</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>6833.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.23</td>\n",
       "      <td>23.99</td>\n",
       "      <td>91.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>19993.0</td>\n",
       "      <td>55621.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.5</td>\n",
       "      <td>2826.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>41.0</td>\n",
       "      <td>7.13</td>\n",
       "      <td>19.99</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>11922.0</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.4</td>\n",
       "      <td>991.0</td>\n",
       "      <td>20.67</td>\n",
       "      <td>91.0</td>\n",
       "      <td>5.23</td>\n",
       "      <td>28.99</td>\n",
       "      <td>77.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2193.0</td>\n",
       "      <td>7422.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.2</td>\n",
       "      <td>681.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>57.0</td>\n",
       "      <td>5.05</td>\n",
       "      <td>16.99</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>25989.0</td>\n",
       "      <td>242683.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.3</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>13.33</td>\n",
       "      <td>142.0</td>\n",
       "      <td>10.77</td>\n",
       "      <td>20.99</td>\n",
       "      <td>55.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>25581.0</td>\n",
       "      <td>113480.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_rating  number_ratings  last_updated  lectures  duration  price2  \\\n",
       "0         4.6          6833.0         20.83      53.0      5.23   23.99   \n",
       "1         4.5          2826.0         20.83      41.0      7.13   19.99   \n",
       "2         4.4           991.0         20.67      91.0      5.23   28.99   \n",
       "3         4.2           681.0         20.83      57.0      5.05   16.99   \n",
       "4         4.3          1939.0         13.33     142.0     10.77   20.99   \n",
       "\n",
       "   discount  inst_rating  inst_review  inst_student  inst_course  cat_design  \\\n",
       "0      91.0          4.6      19993.0       55621.0          3.0           0   \n",
       "1      88.0          4.5      11922.0       53659.0         10.0           0   \n",
       "2      77.0          4.4       2193.0        7422.0          6.0           0   \n",
       "3      64.0          4.3      25989.0      242683.0         43.0           0   \n",
       "4      55.0          4.5      25581.0      113480.0         12.0           0   \n",
       "\n",
       "   cat_development  cat_hobby  cat_it_software  \n",
       "0                1          0                0  \n",
       "1                0          0                0  \n",
       "2                0          0                0  \n",
       "3                0          0                1  \n",
       "4                0          0                0  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## dataframe without target\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c4a37f",
   "metadata": {},
   "source": [
    "### 5.3) Linear Regression model with All features and Log1P transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b6001305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.6255\n",
      "RMSE =  5475.518280457208\n"
     ]
    }
   ],
   "source": [
    "log = lambda x: np.log1p(x)\n",
    "# log transformation of all features\n",
    "features_log = df2.apply(log)\n",
    "features_log\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_log,target, test_size=0.25, \n",
    "   random_state=42)\n",
    "# building the model using original features and Log transformation\n",
    "model_Log = LinearRegression()\n",
    "model_Log.fit(X_train, Y_train)\n",
    "\n",
    "# get the predictions for the test dataset using original features and Log transformation\n",
    "Y_pred = model_Log.predict(X_test)\n",
    "\n",
    "# calculating R2 metrics scores\n",
    "r2_log = model_Log.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_log)\n",
    "\n",
    "# calculating RMSE metrics scores\n",
    "mse2_log = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_log**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4f3781f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the following information to the lists\n",
    "feature_selection.append(\"All\")\n",
    "feature_transformation.append(\"Log1P\")\n",
    "feature_scaling.append(\"None\")\n",
    "r2_values.append(r2_log)\n",
    "rmse_values.append(mse2_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e9dbac",
   "metadata": {},
   "source": [
    "### 5.4) Linear Regression model with All features and MinMax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "04c2b16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>number_ratings</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>lectures</th>\n",
       "      <th>duration</th>\n",
       "      <th>price2</th>\n",
       "      <th>discount</th>\n",
       "      <th>inst_rating</th>\n",
       "      <th>inst_review</th>\n",
       "      <th>inst_student</th>\n",
       "      <th>inst_course</th>\n",
       "      <th>cat_design</th>\n",
       "      <th>cat_development</th>\n",
       "      <th>cat_hobby</th>\n",
       "      <th>cat_it_software</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.700442</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069465</td>\n",
       "      <td>0.034003</td>\n",
       "      <td>0.038251</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.190154</td>\n",
       "      <td>0.125811</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.288665</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052098</td>\n",
       "      <td>0.049459</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.113271</td>\n",
       "      <td>0.121369</td>\n",
       "      <td>0.006342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.100092</td>\n",
       "      <td>0.982533</td>\n",
       "      <td>0.124457</td>\n",
       "      <td>0.034003</td>\n",
       "      <td>0.065574</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.020595</td>\n",
       "      <td>0.016680</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.068236</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.075253</td>\n",
       "      <td>0.032539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.247271</td>\n",
       "      <td>0.549351</td>\n",
       "      <td>0.029598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.197513</td>\n",
       "      <td>0.181223</td>\n",
       "      <td>0.198263</td>\n",
       "      <td>0.079069</td>\n",
       "      <td>0.021858</td>\n",
       "      <td>0.403226</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.243384</td>\n",
       "      <td>0.256813</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.091358</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.054993</td>\n",
       "      <td>0.030098</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.021528</td>\n",
       "      <td>0.037535</td>\n",
       "      <td>0.006342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.033707</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007236</td>\n",
       "      <td>0.039453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.087818</td>\n",
       "      <td>0.360395</td>\n",
       "      <td>0.012685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069469</td>\n",
       "      <td>0.991266</td>\n",
       "      <td>0.021708</td>\n",
       "      <td>0.016920</td>\n",
       "      <td>0.027322</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.006306</td>\n",
       "      <td>0.007243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.353715</td>\n",
       "      <td>0.418122</td>\n",
       "      <td>0.036179</td>\n",
       "      <td>0.004311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.822581</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.086609</td>\n",
       "      <td>0.144880</td>\n",
       "      <td>0.017618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.465420</td>\n",
       "      <td>0.963974</td>\n",
       "      <td>0.379161</td>\n",
       "      <td>0.257708</td>\n",
       "      <td>0.289617</td>\n",
       "      <td>0.338710</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.376603</td>\n",
       "      <td>0.339177</td>\n",
       "      <td>0.158562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      avg_rating  number_ratings  last_updated  lectures  duration    price2  \\\n",
       "0       0.727273        0.700442      1.000000  0.069465  0.034003  0.038251   \n",
       "1       0.636364        0.288665      1.000000  0.052098  0.049459  0.016393   \n",
       "2       0.545455        0.100092      0.982533  0.124457  0.034003  0.065574   \n",
       "3       0.363636        0.068236      1.000000  0.075253  0.032539  0.000000   \n",
       "4       0.454545        0.197513      0.181223  0.198263  0.079069  0.021858   \n",
       "...          ...             ...           ...       ...       ...       ...   \n",
       "1450    0.545455        0.091358      1.000000  0.054993  0.030098  0.005464   \n",
       "1451    0.545455        0.033707      1.000000  0.007236  0.039453  0.000000   \n",
       "1452    0.000000        0.069469      0.991266  0.021708  0.016920  0.027322   \n",
       "1453    0.363636        0.353715      0.418122  0.036179  0.004311  0.000000   \n",
       "1454    0.636364        0.465420      0.963974  0.379161  0.257708  0.289617   \n",
       "\n",
       "      discount  inst_rating  inst_review  inst_student  inst_course  \\\n",
       "0     0.983871     0.882353     0.190154      0.125811     0.001409   \n",
       "1     0.935484     0.823529     0.113271      0.121369     0.006342   \n",
       "2     0.758065     0.764706     0.020595      0.016680     0.003524   \n",
       "3     0.548387     0.705882     0.247271      0.549351     0.029598   \n",
       "4     0.403226     0.823529     0.243384      0.256813     0.007752   \n",
       "...        ...          ...          ...           ...          ...   \n",
       "1450  0.903226     0.764706     0.021528      0.037535     0.006342   \n",
       "1451  0.951613     0.647059     0.087818      0.360395     0.012685   \n",
       "1452  0.967742     0.411765     0.006306      0.007243     0.000000   \n",
       "1453  0.822581     0.647059     0.086609      0.144880     0.017618   \n",
       "1454  0.338710     0.764706     0.376603      0.339177     0.158562   \n",
       "\n",
       "      cat_design  cat_development  cat_hobby  cat_it_software  \n",
       "0            0.0              1.0        0.0              0.0  \n",
       "1            0.0              0.0        0.0              0.0  \n",
       "2            0.0              0.0        0.0              0.0  \n",
       "3            0.0              0.0        0.0              1.0  \n",
       "4            0.0              0.0        0.0              0.0  \n",
       "...          ...              ...        ...              ...  \n",
       "1450         0.0              0.0        0.0              0.0  \n",
       "1451         0.0              0.0        0.0              1.0  \n",
       "1452         0.0              0.0        0.0              0.0  \n",
       "1453         0.0              0.0        0.0              0.0  \n",
       "1454         0.0              0.0        0.0              1.0  \n",
       "\n",
       "[1455 rows x 15 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms = MinMaxScaler()\n",
    "scaled_features = ms.fit_transform(features)\n",
    "\n",
    "#  new dataframe\n",
    "scaled_Features_All = pd.DataFrame(scaled_features, columns = features.columns)\n",
    "scaled_Features_All\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4cd7bfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.7526\n",
      "RMSE =  4450.289645095106\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(scaled_Features_All, target, test_size=0.25, \n",
    "   random_state=42)\n",
    "# building the model using original features and minMax scaling\n",
    "model_MinMax = LinearRegression()\n",
    "model_MinMax.fit(X_train, Y_train)\n",
    "\n",
    "# get the predictions for the test dataset using original features and minMax scaling\n",
    "Y_pred = model_MinMax.predict(X_test)\n",
    "\n",
    "# calculating R2 metrics scores\n",
    "r2_minMax = model_MinMax.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_minMax)\n",
    "\n",
    "# calculating RMSE metrics scores\n",
    "mse2_minMax = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_minMax**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "59a3daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the following information to the lists\n",
    "feature_selection.append(\"All\")\n",
    "feature_transformation.append(\"None\")\n",
    "feature_scaling.append(\"MinMax\")\n",
    "r2_values.append(r2_minMax)\n",
    "rmse_values.append(mse2_minMax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa7b4e",
   "metadata": {},
   "source": [
    "### 5.5) Linear Regression model with All features, Polynomial Features and MinMax Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "362f04e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>number_ratings</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>lectures</th>\n",
       "      <th>duration</th>\n",
       "      <th>price2</th>\n",
       "      <th>discount</th>\n",
       "      <th>inst_rating</th>\n",
       "      <th>inst_review</th>\n",
       "      <th>inst_student</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_design^2</th>\n",
       "      <th>cat_design cat_development</th>\n",
       "      <th>cat_design cat_hobby</th>\n",
       "      <th>cat_design cat_it_software</th>\n",
       "      <th>cat_development^2</th>\n",
       "      <th>cat_development cat_hobby</th>\n",
       "      <th>cat_development cat_it_software</th>\n",
       "      <th>cat_hobby^2</th>\n",
       "      <th>cat_hobby cat_it_software</th>\n",
       "      <th>cat_it_software^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.700442</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069465</td>\n",
       "      <td>0.034003</td>\n",
       "      <td>0.038251</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.190154</td>\n",
       "      <td>0.125811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.288665</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052098</td>\n",
       "      <td>0.049459</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.113271</td>\n",
       "      <td>0.121369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.100092</td>\n",
       "      <td>0.982533</td>\n",
       "      <td>0.124457</td>\n",
       "      <td>0.034003</td>\n",
       "      <td>0.065574</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.020595</td>\n",
       "      <td>0.016680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.068236</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.075253</td>\n",
       "      <td>0.032539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.247271</td>\n",
       "      <td>0.549351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.197513</td>\n",
       "      <td>0.181223</td>\n",
       "      <td>0.198263</td>\n",
       "      <td>0.079069</td>\n",
       "      <td>0.021858</td>\n",
       "      <td>0.403226</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.243384</td>\n",
       "      <td>0.256813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.091358</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.054993</td>\n",
       "      <td>0.030098</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.021528</td>\n",
       "      <td>0.037535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.033707</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007236</td>\n",
       "      <td>0.039453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.087818</td>\n",
       "      <td>0.360395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069469</td>\n",
       "      <td>0.991266</td>\n",
       "      <td>0.021708</td>\n",
       "      <td>0.016920</td>\n",
       "      <td>0.027322</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.006306</td>\n",
       "      <td>0.007243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.353715</td>\n",
       "      <td>0.418122</td>\n",
       "      <td>0.036179</td>\n",
       "      <td>0.004311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.822581</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.086609</td>\n",
       "      <td>0.144880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.465420</td>\n",
       "      <td>0.963974</td>\n",
       "      <td>0.379161</td>\n",
       "      <td>0.257708</td>\n",
       "      <td>0.289617</td>\n",
       "      <td>0.338710</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.376603</td>\n",
       "      <td>0.339177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455 rows × 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      avg_rating  number_ratings  last_updated  lectures  duration    price2  \\\n",
       "0       0.727273        0.700442      1.000000  0.069465  0.034003  0.038251   \n",
       "1       0.636364        0.288665      1.000000  0.052098  0.049459  0.016393   \n",
       "2       0.545455        0.100092      0.982533  0.124457  0.034003  0.065574   \n",
       "3       0.363636        0.068236      1.000000  0.075253  0.032539  0.000000   \n",
       "4       0.454545        0.197513      0.181223  0.198263  0.079069  0.021858   \n",
       "...          ...             ...           ...       ...       ...       ...   \n",
       "1450    0.545455        0.091358      1.000000  0.054993  0.030098  0.005464   \n",
       "1451    0.545455        0.033707      1.000000  0.007236  0.039453  0.000000   \n",
       "1452    0.000000        0.069469      0.991266  0.021708  0.016920  0.027322   \n",
       "1453    0.363636        0.353715      0.418122  0.036179  0.004311  0.000000   \n",
       "1454    0.636364        0.465420      0.963974  0.379161  0.257708  0.289617   \n",
       "\n",
       "      discount  inst_rating  inst_review  inst_student  ...  cat_design^2  \\\n",
       "0     0.983871     0.882353     0.190154      0.125811  ...           0.0   \n",
       "1     0.935484     0.823529     0.113271      0.121369  ...           0.0   \n",
       "2     0.758065     0.764706     0.020595      0.016680  ...           0.0   \n",
       "3     0.548387     0.705882     0.247271      0.549351  ...           0.0   \n",
       "4     0.403226     0.823529     0.243384      0.256813  ...           0.0   \n",
       "...        ...          ...          ...           ...  ...           ...   \n",
       "1450  0.903226     0.764706     0.021528      0.037535  ...           0.0   \n",
       "1451  0.951613     0.647059     0.087818      0.360395  ...           0.0   \n",
       "1452  0.967742     0.411765     0.006306      0.007243  ...           0.0   \n",
       "1453  0.822581     0.647059     0.086609      0.144880  ...           0.0   \n",
       "1454  0.338710     0.764706     0.376603      0.339177  ...           0.0   \n",
       "\n",
       "      cat_design cat_development  cat_design cat_hobby  \\\n",
       "0                            0.0                   0.0   \n",
       "1                            0.0                   0.0   \n",
       "2                            0.0                   0.0   \n",
       "3                            0.0                   0.0   \n",
       "4                            0.0                   0.0   \n",
       "...                          ...                   ...   \n",
       "1450                         0.0                   0.0   \n",
       "1451                         0.0                   0.0   \n",
       "1452                         0.0                   0.0   \n",
       "1453                         0.0                   0.0   \n",
       "1454                         0.0                   0.0   \n",
       "\n",
       "      cat_design cat_it_software  cat_development^2  \\\n",
       "0                            0.0                1.0   \n",
       "1                            0.0                0.0   \n",
       "2                            0.0                0.0   \n",
       "3                            0.0                0.0   \n",
       "4                            0.0                0.0   \n",
       "...                          ...                ...   \n",
       "1450                         0.0                0.0   \n",
       "1451                         0.0                0.0   \n",
       "1452                         0.0                0.0   \n",
       "1453                         0.0                0.0   \n",
       "1454                         0.0                0.0   \n",
       "\n",
       "      cat_development cat_hobby  cat_development cat_it_software  cat_hobby^2  \\\n",
       "0                           0.0                              0.0          0.0   \n",
       "1                           0.0                              0.0          0.0   \n",
       "2                           0.0                              0.0          0.0   \n",
       "3                           0.0                              0.0          0.0   \n",
       "4                           0.0                              0.0          0.0   \n",
       "...                         ...                              ...          ...   \n",
       "1450                        0.0                              0.0          0.0   \n",
       "1451                        0.0                              0.0          0.0   \n",
       "1452                        0.0                              0.0          0.0   \n",
       "1453                        0.0                              0.0          0.0   \n",
       "1454                        0.0                              0.0          0.0   \n",
       "\n",
       "      cat_hobby cat_it_software  cat_it_software^2  \n",
       "0                           0.0                0.0  \n",
       "1                           0.0                0.0  \n",
       "2                           0.0                0.0  \n",
       "3                           0.0                1.0  \n",
       "4                           0.0                0.0  \n",
       "...                         ...                ...  \n",
       "1450                        0.0                0.0  \n",
       "1451                        0.0                1.0  \n",
       "1452                        0.0                0.0  \n",
       "1453                        0.0                0.0  \n",
       "1454                        0.0                1.0  \n",
       "\n",
       "[1455 rows x 135 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using a polynomial degree of 2 (without bias) and MinMax scaling\n",
    "pf_f = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "pf = pf_f.fit(features)\n",
    "\n",
    "poly_f= pd.DataFrame(pf_f.transform(features), columns = pf.get_feature_names(features.columns))\n",
    "poly_f\n",
    "\n",
    "# minmax scaling\n",
    "ms_poly = MinMaxScaler()\n",
    "scaled_features = ms_poly.fit_transform(poly_f)\n",
    "\n",
    "# renaming dataframe\n",
    "features_MinMax_pm = pd.DataFrame(scaled_features, columns = poly_f.columns)\n",
    "features_MinMax_pm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "90ea65d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.5838\n",
      "RMSE =  5771.943617309929\n"
     ]
    }
   ],
   "source": [
    "# fitting the model\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_MinMax_pm, target, test_size=0.25, \n",
    "   random_state=42)\n",
    "\n",
    "# building the model using original features\n",
    "model_MinMax_pm = LinearRegression()\n",
    "model_MinMax_pm.fit(X_train, Y_train)\n",
    "\n",
    "# get the predictions for the test dataset\n",
    "Y_pred = model_MinMax_pm.predict(X_test)\n",
    "\n",
    "# R-squared metric scores\n",
    "r2_minMax_pm = model_MinMax_pm.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_minMax_pm)\n",
    "\n",
    "# RMSE metric scores\n",
    "mse2_minMax_pm = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_minMax_pm**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3b75337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending the following information to the lists\n",
    "feature_selection.append(\"All\")\n",
    "feature_transformation.append(\"Poly 2\")\n",
    "feature_scaling.append(\"MinMax\")\n",
    "r2_values.append(r2_minMax_pm)\n",
    "rmse_values.append(mse2_minMax_pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdad9e9",
   "metadata": {},
   "source": [
    "### 5.6) Linear Regression model with All features, Log1P Transformation and MinMax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8c1b699c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>number_ratings</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>lectures</th>\n",
       "      <th>duration</th>\n",
       "      <th>price2</th>\n",
       "      <th>discount</th>\n",
       "      <th>inst_rating</th>\n",
       "      <th>inst_review</th>\n",
       "      <th>inst_student</th>\n",
       "      <th>inst_course</th>\n",
       "      <th>cat_design</th>\n",
       "      <th>cat_development</th>\n",
       "      <th>cat_hobby</th>\n",
       "      <th>cat_it_software</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.747084</td>\n",
       "      <td>0.943562</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.462085</td>\n",
       "      <td>0.270426</td>\n",
       "      <td>0.136179</td>\n",
       "      <td>0.990159</td>\n",
       "      <td>0.898835</td>\n",
       "      <td>0.795132</td>\n",
       "      <td>0.769076</td>\n",
       "      <td>0.105567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.659758</td>\n",
       "      <td>0.803330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.409232</td>\n",
       "      <td>0.335186</td>\n",
       "      <td>0.063905</td>\n",
       "      <td>0.959983</td>\n",
       "      <td>0.846889</td>\n",
       "      <td>0.731279</td>\n",
       "      <td>0.765074</td>\n",
       "      <td>0.259634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.570830</td>\n",
       "      <td>0.636956</td>\n",
       "      <td>0.986478</td>\n",
       "      <td>0.574135</td>\n",
       "      <td>0.270426</td>\n",
       "      <td>0.211751</td>\n",
       "      <td>0.839897</td>\n",
       "      <td>0.793990</td>\n",
       "      <td>0.522197</td>\n",
       "      <td>0.544628</td>\n",
       "      <td>0.190796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.387923</td>\n",
       "      <td>0.577430</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.477113</td>\n",
       "      <td>0.263293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.673941</td>\n",
       "      <td>0.740102</td>\n",
       "      <td>0.827528</td>\n",
       "      <td>0.933254</td>\n",
       "      <td>0.470767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.480239</td>\n",
       "      <td>0.743512</td>\n",
       "      <td>0.226300</td>\n",
       "      <td>0.666891</td>\n",
       "      <td>0.425202</td>\n",
       "      <td>0.083189</td>\n",
       "      <td>0.538283</td>\n",
       "      <td>0.846889</td>\n",
       "      <td>0.825574</td>\n",
       "      <td>0.848543</td>\n",
       "      <td>0.285076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_rating  number_ratings  last_updated  lectures  duration    price2  \\\n",
       "0    0.747084        0.943562      1.000000  0.462085  0.270426  0.136179   \n",
       "1    0.659758        0.803330      1.000000  0.409232  0.335186  0.063905   \n",
       "2    0.570830        0.636956      0.986478  0.574135  0.270426  0.211751   \n",
       "3    0.387923        0.577430      1.000000  0.477113  0.263293  0.000000   \n",
       "4    0.480239        0.743512      0.226300  0.666891  0.425202  0.083189   \n",
       "\n",
       "   discount  inst_rating  inst_review  inst_student  inst_course  cat_design  \\\n",
       "0  0.990159     0.898835     0.795132      0.769076     0.105567         0.0   \n",
       "1  0.959983     0.846889     0.731279      0.765074     0.259634         0.0   \n",
       "2  0.839897     0.793990     0.522197      0.544628     0.190796         0.0   \n",
       "3  0.673941     0.740102     0.827528      0.933254     0.470767         0.0   \n",
       "4  0.538283     0.846889     0.825574      0.848543     0.285076         0.0   \n",
       "\n",
       "   cat_development  cat_hobby  cat_it_software  \n",
       "0              1.0        0.0              0.0  \n",
       "1              0.0        0.0              0.0  \n",
       "2              0.0        0.0              0.0  \n",
       "3              0.0        0.0              1.0  \n",
       "4              0.0        0.0              0.0  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Linear Regression model with All features, Log1P Transformation and MinMax Scaling\n",
    "log = lambda x: np.log1p(x)\n",
    "# log transformation of all features\n",
    "features_logMin = df2.apply(log)\n",
    "features_logMin\n",
    "\n",
    "# minmax scaling\n",
    "ms_logMin = MinMaxScaler()\n",
    "scaled_features = ms_logMin.fit_transform(features_logMin)\n",
    "\n",
    "# renaming dataframe\n",
    "features_MinMax_lm = pd.DataFrame(scaled_features, columns = features_logMin.columns)\n",
    "features_MinMax_lm.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e82ab226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.6255\n",
      "RMSE =  5475.518280457207\n"
     ]
    }
   ],
   "source": [
    "# fitting the model\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_MinMax_lm, target, test_size=0.25, \n",
    "   random_state=42)\n",
    "\n",
    "# building the model using original features\n",
    "model_MinMax_lm = LinearRegression()\n",
    "model_MinMax_lm.fit(X_train, Y_train)\n",
    "\n",
    "# getting the predictions for the test dataset\n",
    "Y_pred = model_MinMax_lm.predict(X_test)\n",
    "\n",
    "# R-squared metric  scores\n",
    "r2_minMax_lm = model_MinMax_lm.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_minMax_lm)\n",
    "\n",
    "# RMSE metric scores\n",
    "mse2_minMax_lm = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_minMax_lm**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "619a8ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending the following information to the lists\n",
    "feature_selection.append(\"All\")\n",
    "feature_transformation.append(\"Log1P\")\n",
    "feature_scaling.append(\"MinMax\")\n",
    "r2_values.append(r2_minMax_lm)\n",
    "rmse_values.append(mse2_minMax_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42603591",
   "metadata": {},
   "source": [
    "### 5.h.1 Linear Regression model with Manual Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a9de61e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_ratings</th>\n",
       "      <th>inst_student</th>\n",
       "      <th>inst_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6833.0</td>\n",
       "      <td>55621.0</td>\n",
       "      <td>19993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2826.0</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>11922.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>991.0</td>\n",
       "      <td>7422.0</td>\n",
       "      <td>2193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>681.0</td>\n",
       "      <td>242683.0</td>\n",
       "      <td>25989.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1939.0</td>\n",
       "      <td>113480.0</td>\n",
       "      <td>25581.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_ratings  inst_student  inst_review\n",
       "0          6833.0       55621.0      19993.0\n",
       "1          2826.0       53659.0      11922.0\n",
       "2           991.0        7422.0       2193.0\n",
       "3           681.0      242683.0      25989.0\n",
       "4          1939.0      113480.0      25581.0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using dataframe obtained in step 4\n",
    "df1_manual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "54b57bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.7507\n",
      "RMSE =  4467.404078359277\n"
     ]
    }
   ],
   "source": [
    "# response variable\n",
    "target = df1.enrollment\n",
    "# fitting model\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df1_manual,target, test_size=0.25, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# building the model using manual selection\n",
    "model_manual = LinearRegression()\n",
    "model_manual.fit(X_train, Y_train)\n",
    "\n",
    "# predicting the test dataset\n",
    "Y_pred = model_manual.predict(X_test)\n",
    "\n",
    "# R-squared metric scores\n",
    "r2_manual = model_manual.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_manual)\n",
    "\n",
    "# RMSE metric scores\n",
    "mse2_manual = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_manual**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "99398778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending the following information to the lists\n",
    "feature_selection.append(\"Manual\")\n",
    "feature_transformation.append(\"None\")\n",
    "feature_scaling.append(\"None\")\n",
    "r2_values.append(r2_manual)\n",
    "rmse_values.append(mse2_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f9ecbf",
   "metadata": {},
   "source": [
    "### 5.h.2) Linear Regression model with manual selection and Polynomial Features transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ae7db7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared =  0.7488\n",
      "RMSE =  4484.156850576802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "pff = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "pf = pff.fit(df1_manual)\n",
    "\n",
    "# using the get_feature_names for the column name\n",
    "poly_manual= pd.DataFrame(pff.transform(df1_manual), columns = pf.get_feature_names(df1_manual.columns))\n",
    "poly_manual\n",
    "#response variable\n",
    "target = df1.enrollment\n",
    "\n",
    "# fitting model\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(poly_manual,target, test_size=0.25, random_state=42) \n",
    "      \n",
    "# building the model\n",
    "modelPolynomial_manual = LinearRegression()\n",
    "modelPolynomial_manual.fit(X_train, Y_train)    \n",
    "\n",
    "# predicting the test dataset\n",
    "Y_pred = modelPolynomial_manual.predict(X_test)\n",
    "# R-squared metric scores\n",
    "r2_polyManual = modelPolynomial_manual.score(X_test, Y_test).round(4)\n",
    "print(\"R squared = \", r2_polyManual)\n",
    "\n",
    "# RMSE metric scores\n",
    "mse_polyManual = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \", mse_polyManual**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "423eac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending the following information to the lists\n",
    "feature_selection.append(\"Manual\")\n",
    "feature_transformation.append(\"Poly 2\")\n",
    "feature_scaling.append(\"None\")\n",
    "r2_values.append(r2_polyManual)\n",
    "rmse_values.append(mse_polyManual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43460617",
   "metadata": {},
   "source": [
    "### 5.h.3) Linear Regression model with Manual Selection and Log1P transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "37e6c95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.627\n",
      "RMSE =  5464.327063240488\n"
     ]
    }
   ],
   "source": [
    "# using log transformation\n",
    "log = lambda x: np.log1p(x)\n",
    "# log of all selected features\n",
    "features_logManual = df1_manual.apply(log)\n",
    "features_logManual\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_logManual,target, test_size=0.25, \n",
    "   random_state=42)\n",
    "\n",
    "# fitting the model\n",
    "model_LogManual = LinearRegression()\n",
    "model_LogManual.fit(X_train, Y_train)\n",
    "\n",
    "# get the predictions for the test dataset\n",
    "Y_pred = model_LogManual.predict(X_test)\n",
    "\n",
    "# R-squared metric scores\n",
    "r2_logManual = model_LogManual.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_logManual)\n",
    "\n",
    "# RMSE metric scores\n",
    "mse2_logManual = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_logManual**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5225e08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending the following information to the lists\n",
    "feature_selection.append(\"Manual\")\n",
    "feature_transformation.append(\"Log1P\")\n",
    "feature_scaling.append(\"None\")\n",
    "r2_values.append(r2_logManual)\n",
    "rmse_values.append(mse2_logManual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25417989",
   "metadata": {},
   "source": [
    "### 5.h.4) Linear Regression model with Manual Selection and MinMax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3166f0d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.7507\n",
      "RMSE =  4467.404078359277\n"
     ]
    }
   ],
   "source": [
    "ms = MinMaxScaler()\n",
    "scaled_features = ms.fit_transform(df1_manual)\n",
    "\n",
    "# creating the dataframe\n",
    "df1_manual = pd.DataFrame(scaled_features, columns = df1_manual.columns)\n",
    "features_MinMax_Manual\n",
    "#fitting model\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_MinMax_Manual, target, test_size=0.25, \n",
    "   random_state=42)\n",
    "\n",
    "# linear regression model\n",
    "model_MinMaxManual = LinearRegression()\n",
    "model_MinMaxManual.fit(X_train, Y_train)\n",
    "\n",
    "# getting the predictions for the test dataset using manual selection\n",
    "Y_pred = model_MinMaxManual.predict(X_test)\n",
    "\n",
    "# R-squared metric scores\n",
    "r2_minMaxManual = model_MinMaxManual.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_minMaxManual)\n",
    "\n",
    "# RMSE metric scores\n",
    "mse2_minMaxManual = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_minMaxManual**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "24442757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending the following information to the lists\n",
    "feature_selection.append(\"Manual\")\n",
    "feature_transformation.append(\"None\")\n",
    "feature_scaling.append(\"MinMax\")\n",
    "r2_values.append(r2_minMaxManual)\n",
    "rmse_values.append(mse2_minMaxManual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd542dc",
   "metadata": {},
   "source": [
    "### 5.h.5) Linear Regression model with Manual Selection, Polynomial Features and MinMax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a8c56845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.7488\n",
      "RMSE =  4484.156850518718\n"
     ]
    }
   ],
   "source": [
    "#polynomial transformation\n",
    "pf_manual = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "pf = pf_manual.fit(df1_manual)\n",
    "poly_manual1= pd.DataFrame(pf_manual.transform(df1_manual), columns = pf.get_feature_names(df1_manual.columns))\n",
    "poly_manual1\n",
    "# response variable\n",
    "target = df1.enrollment\n",
    "\n",
    "# minmax scaling\n",
    "ms_polyManual = MinMaxScaler()\n",
    "scaled_features = ms_polyManual.fit_transform(poly_manual1)\n",
    "\n",
    "# remaking the dataframe\n",
    "features_MinMax_pmin = pd.DataFrame(scaled_features, columns = poly_manual1.columns)\n",
    "features_MinMax_pmin\n",
    "\n",
    "# fitting model\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_MinMax_pmin, target, test_size=0.25, \n",
    "   random_state=42)\n",
    "\n",
    "model_MinMax_pmm = LinearRegression()\n",
    "model_MinMax_pmm.fit(X_train, Y_train)\n",
    "\n",
    "# getting the predictions for the test dataset\n",
    "Y_pred = model_MinMax_pmm.predict(X_test)\n",
    "\n",
    "# R-squared metirc scores\n",
    "r2_minMax_pmm = model_MinMax_pmm.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_minMax_pmm)\n",
    "\n",
    "# RMSE metric scores\n",
    "mse2_minMax_pmm = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_minMax_pmm**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "79d4300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending the following information to the lists\n",
    "feature_selection.append(\"Manual\")\n",
    "feature_transformation.append(\"Poly 2\")\n",
    "feature_scaling.append(\"MinMax\")\n",
    "r2_values.append(r2_minMax_pmm)\n",
    "rmse_values.append(mse2_minMax_pmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca03e22",
   "metadata": {},
   "source": [
    "### 5.h.6) Linear Regression model with Manual Selection, Log1P Transformation and MinMax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e76d9b15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.6255\n",
      "RMSE =  5475.518280457207\n"
     ]
    }
   ],
   "source": [
    "#log of features\n",
    "log = lambda x: np.log1p(x)\n",
    "features_logMinManual = df1_manual.apply(log)\n",
    "features_logMinManual\n",
    "\n",
    "# minmax scaling\n",
    "ms_logMinManual = MinMaxScaler()\n",
    "scaled_features = ms_logMinManual.fit_transform(features_logMinManual)\n",
    "\n",
    "# remaking the dataframe\n",
    "features_MinMax_lmm = pd.DataFrame(scaled_features, columns = features_logMinManual.columns)\n",
    "features_MinMax_lmm\n",
    "# fitting model\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_MinMax_lm, target, test_size=0.25, \n",
    "   random_state=42)\n",
    "\n",
    "# linear regression model\n",
    "model_MinMax_lmm = LinearRegression()\n",
    "model_MinMax_lmm.fit(X_train, Y_train)\n",
    "\n",
    "# getting the predictions for the test dataset\n",
    "Y_pred = model_MinMax_lmm.predict(X_test)\n",
    "\n",
    "# R-squared metric score\n",
    "r2_minMax_lmm = model_MinMax_lmm.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_minMax_lmm)\n",
    "\n",
    "# RMSE metric score\n",
    "mse2_minMax_lmm = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_minMax_lmm**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2144f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending the following information to the lists\n",
    "feature_selection.append(\"Manual\")\n",
    "feature_transformation.append(\"Log1P\")\n",
    "feature_scaling.append(\"MinMax\")\n",
    "r2_values.append(r2_minMax_lmm)\n",
    "rmse_values.append(mse2_minMax_lmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf877af",
   "metadata": {},
   "source": [
    "### 5.i.1) Linear Regression model with Variance Threshold Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6cd408a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_ratings</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>lectures</th>\n",
       "      <th>duration</th>\n",
       "      <th>price2</th>\n",
       "      <th>discount</th>\n",
       "      <th>inst_review</th>\n",
       "      <th>inst_student</th>\n",
       "      <th>inst_course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6833.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.23</td>\n",
       "      <td>23.99</td>\n",
       "      <td>91.0</td>\n",
       "      <td>19993.0</td>\n",
       "      <td>55621.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2826.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>41.0</td>\n",
       "      <td>7.13</td>\n",
       "      <td>19.99</td>\n",
       "      <td>88.0</td>\n",
       "      <td>11922.0</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>991.0</td>\n",
       "      <td>20.67</td>\n",
       "      <td>91.0</td>\n",
       "      <td>5.23</td>\n",
       "      <td>28.99</td>\n",
       "      <td>77.0</td>\n",
       "      <td>2193.0</td>\n",
       "      <td>7422.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>681.0</td>\n",
       "      <td>20.83</td>\n",
       "      <td>57.0</td>\n",
       "      <td>5.05</td>\n",
       "      <td>16.99</td>\n",
       "      <td>64.0</td>\n",
       "      <td>25989.0</td>\n",
       "      <td>242683.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1939.0</td>\n",
       "      <td>13.33</td>\n",
       "      <td>142.0</td>\n",
       "      <td>10.77</td>\n",
       "      <td>20.99</td>\n",
       "      <td>55.0</td>\n",
       "      <td>25581.0</td>\n",
       "      <td>113480.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_ratings  last_updated  lectures  duration  price2  discount  \\\n",
       "0          6833.0         20.83      53.0      5.23   23.99      91.0   \n",
       "1          2826.0         20.83      41.0      7.13   19.99      88.0   \n",
       "2           991.0         20.67      91.0      5.23   28.99      77.0   \n",
       "3           681.0         20.83      57.0      5.05   16.99      64.0   \n",
       "4          1939.0         13.33     142.0     10.77   20.99      55.0   \n",
       "\n",
       "   inst_review  inst_student  inst_course  \n",
       "0      19993.0       55621.0          3.0  \n",
       "1      11922.0       53659.0         10.0  \n",
       "2       2193.0        7422.0          6.0  \n",
       "3      25989.0      242683.0         43.0  \n",
       "4      25581.0      113480.0         12.0  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe obtained at step 4.c\n",
    "df_vt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a6b4e314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.7497\n",
      "RMSE =  4476.089110476118\n"
     ]
    }
   ],
   "source": [
    "target = df1.enrollment\n",
    "# fitting model\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df_vt,target, test_size=0.25, \n",
    "                                                    random_state=42)\n",
    "# building the model\n",
    "model_variance = LinearRegression()\n",
    "model_variance.fit(X_train, Y_train)\n",
    "\n",
    "# predicting the test dataset using Variance Threshold Selection\n",
    "Y_pred = model_variance.predict(X_test)\n",
    "\n",
    "# R-squared metric score\n",
    "r2_variance = model_variance.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_variance)\n",
    "\n",
    "# RMSE metric score\n",
    "mse2_variance = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_variance**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5f81b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the following information to the lists\n",
    "feature_selection.append(\"VT\")\n",
    "feature_transformation.append(\"None\")\n",
    "feature_scaling.append(\"None\")\n",
    "r2_values.append(r2_variance)\n",
    "rmse_values.append(mse2_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df72b5e5",
   "metadata": {},
   "source": [
    "### 5.i.2) Linear Regression model with Variance Threshold and Polynomial Features transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3aaa7e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared is  0.6983\n",
      "RMSE is  4914.76859411067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Polynomial Features transformation\n",
    "pf_variance = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "pf_vari = pf_variance.fit(df_vt)\n",
    "\n",
    "# using the get_feature_names for the column name\n",
    "poly_variance= pd.DataFrame(pf_variance.transform(df_vt), columns = pf_vari.get_feature_names(df_vt.columns))\n",
    "poly_variance\n",
    "\n",
    "target = df1.enrollment\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(poly_variance,target, test_size=0.25, random_state=42) \n",
    "      \n",
    "# building the model\n",
    "modelPolynomial_variance = LinearRegression()\n",
    "modelPolynomial_variance.fit(X_train, Y_train)    \n",
    "\n",
    "# predicting the test dataset\n",
    "Y_pred = modelPolynomial_variance.predict(X_test)\n",
    "# R-squared metric score\n",
    "r2_poly_variance = modelPolynomial_variance.score(X_test, Y_test).round(4)\n",
    "print(\"R squared is \", r2_poly_variance)\n",
    "\n",
    "# RMSE metric score\n",
    "mse_poly_variance = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE is \", mse_poly_variance**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "57243280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the following information to the lists\n",
    "feature_selection.append(\"VT\")\n",
    "feature_transformation.append(\"Poly 2\")\n",
    "feature_scaling.append(\"None\")\n",
    "r2_values.append(r2_poly_variance)\n",
    "rmse_values.append(mse_poly_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d7dc5e",
   "metadata": {},
   "source": [
    "### 5.i.3) Linear Regression with Variance Threshold and Log1P transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2789acbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.6233\n",
      "RMSE =  5491.111305408759\n"
     ]
    }
   ],
   "source": [
    "# log transformation\n",
    "log = lambda x: np.log1p(x)\n",
    "\n",
    "features_logVariance = df_vt.apply(log)\n",
    "features_logVariance\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_logVariance,target, test_size=0.25, \n",
    "   random_state=42)\n",
    "# linear regression model\n",
    "model_LogVariance = LinearRegression()\n",
    "model_LogVariance.fit(X_train, Y_train)\n",
    "\n",
    "# getting the predictions for the test dataset\n",
    "Y_pred = model_LogVariance.predict(X_test)\n",
    "\n",
    "# R-squared metric score\n",
    "r2_logVariance = model_LogVariance.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_logVariance)\n",
    "\n",
    "# RMSE metric score\n",
    "mse2_logVariance = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_logVariance**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f0f5ff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the following information to the lists\n",
    "feature_selection.append(\"VT\")\n",
    "feature_transformation.append(\"Log1P\")\n",
    "feature_scaling.append(\"None\")\n",
    "r2_values.append(r2_logVariance)\n",
    "rmse_values.append(mse2_logVariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0430d1c8",
   "metadata": {},
   "source": [
    "### 5.i.4) Linear Regression model with Variance Threshold and MinMax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b158211a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.7497\n",
      "RMSE =  4476.089110475981\n"
     ]
    }
   ],
   "source": [
    "# minMax scaler\n",
    "ms_var = MinMaxScaler()\n",
    "scaled_features = ms_var.fit_transform(df_vt)\n",
    "\n",
    "# Remaking the dataframe\n",
    "features_MinMax_Variance = pd.DataFrame(scaled_features, columns = df_vt.columns)\n",
    "features_MinMax_Variance\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_MinMax_Variance, target, test_size=0.25, \n",
    "   random_state=42)\n",
    "\n",
    "# building the linear regression model\n",
    "model_MinMaxVariance = LinearRegression()\n",
    "model_MinMaxVariance.fit(X_train, Y_train)\n",
    "\n",
    "# getting the predictions for the test dataset\n",
    "Y_pred = model_MinMaxVariance.predict(X_test)\n",
    "\n",
    "# R-squared metric score\n",
    "r2_minMaxVariance = model_MinMaxVariance.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_minMaxVariance)\n",
    "\n",
    "# RMSE metric score\n",
    "mse2_minMaxVariance = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_minMaxVariance**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "de7bcdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the following information to the lists\n",
    "feature_selection.append(\"VT\")\n",
    "feature_transformation.append(\"None\")\n",
    "feature_scaling.append(\"MinMax\")\n",
    "r2_values.append(r2_minMaxVariance)\n",
    "rmse_values.append(mse2_minMaxVariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95c7e84",
   "metadata": {},
   "source": [
    "### 5.i.5) Linear Regression model with Variance Threshold, Polynomial Features and MinMax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9264999d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.6983\n",
      "RMSE =  4914.771345243868\n"
     ]
    }
   ],
   "source": [
    "# polynomial features\n",
    "pf_variance = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "pf = pf_variance.fit(df_vt)\n",
    "\n",
    "# Using the get_feature_names for the column name\n",
    "poly_variance1= pd.DataFrame(pf_variance.transform(df_vt), columns = pf.get_feature_names(df_vt.columns))\n",
    "poly_variance1\n",
    "#response variable\n",
    "target = df1.enrollment\n",
    "\n",
    "# minmax scaling\n",
    "ms_polyvariance = MinMaxScaler()\n",
    "scaled_features = ms_polyvariance.fit_transform(poly_variance1)\n",
    "\n",
    "# Remaking the dataframe\n",
    "features_MinMax_pmin_variance = pd.DataFrame(scaled_features, columns = poly_variance1.columns)\n",
    "features_MinMax_pmin_variance\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_MinMax_pmin_variance, target, test_size=0.25, \n",
    "   random_state=42)\n",
    "# building the linear regression model\n",
    "model_MinMax_pmmvariance = LinearRegression()\n",
    "model_MinMax_pmmvariance.fit(X_train, Y_train)\n",
    "\n",
    "# getting the predictions for the test dataset\n",
    "Y_pred = model_MinMax_pmmvariance.predict(X_test)\n",
    "\n",
    "# R-squared metric score\n",
    "r2_minMax_pmmvariance = model_MinMax_pmmvariance.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_minMax_pmmvariance)\n",
    "\n",
    "# RMSE metric score\n",
    "mse2_minMax_pmmvariance = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_minMax_pmmvariance**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "be748430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the following information to the lists\n",
    "feature_selection.append(\"VT\")\n",
    "feature_transformation.append(\"Poly 2\")\n",
    "feature_scaling.append(\"MinMax\")\n",
    "r2_values.append(r2_minMax_pmmvariance)\n",
    "rmse_values.append(mse2_minMax_pmmvariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e11b26",
   "metadata": {},
   "source": [
    "### 5.i.6) Linear Regression model with Variance Threshold, Log1P Transformation and MinMax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "26b71832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.6233\n",
      "RMSE =  5491.111305408758\n"
     ]
    }
   ],
   "source": [
    "log = lambda x: np.log1p(x)\n",
    "# log transformation of selected features\n",
    "features_logMinVariance = df_vt.apply(log)\n",
    "features_logMinVariance\n",
    "\n",
    "# minmax scaling\n",
    "ms_logMinVariance = MinMaxScaler()\n",
    "scaled_features = ms_logMinVariance.fit_transform(features_logMinVariance)\n",
    "\n",
    "# Remaking the dataframe\n",
    "features_MinMax_lmVariance = pd.DataFrame(scaled_features, columns = features_logMinVariance.columns)\n",
    "features_MinMax_lmVariance\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_MinMax_lmVariance, target, test_size=0.25, \n",
    "   random_state=42)\n",
    "\n",
    "# building the linear regression model \n",
    "model_MinMax_lmVariance = LinearRegression()\n",
    "model_MinMax_lmVariance.fit(X_train, Y_train)\n",
    "\n",
    "# getting the predictions for the test dataset\n",
    "Y_pred = model_MinMax_lmVariance.predict(X_test)\n",
    "\n",
    "# R-squared metric score\n",
    "r2_minMax_lmVariance = model_MinMax_lmVariance.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_minMax_lmVariance)\n",
    "\n",
    "# RMSE metric score\n",
    "mse2_minMax_lmVariance = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_minMax_lmVariance**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cc2b5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the following information to the lists\n",
    "feature_selection.append(\"VT\")\n",
    "feature_transformation.append(\"Log1P\")\n",
    "feature_scaling.append(\"MinMax\")\n",
    "r2_values.append(r2_minMax_lmVariance)\n",
    "rmse_values.append(mse2_minMax_lmVariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb15c9e",
   "metadata": {},
   "source": [
    "### 5.j.1)  Linear Regression model with SelectKBest Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d3262769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_ratings</th>\n",
       "      <th>inst_review</th>\n",
       "      <th>inst_student</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6833.0</td>\n",
       "      <td>19993.0</td>\n",
       "      <td>55621.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2826.0</td>\n",
       "      <td>11922.0</td>\n",
       "      <td>53659.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>991.0</td>\n",
       "      <td>2193.0</td>\n",
       "      <td>7422.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>681.0</td>\n",
       "      <td>25989.0</td>\n",
       "      <td>242683.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1939.0</td>\n",
       "      <td>25581.0</td>\n",
       "      <td>113480.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_ratings  inst_review  inst_student\n",
       "0          6833.0      19993.0       55621.0\n",
       "1          2826.0      11922.0       53659.0\n",
       "2           991.0       2193.0        7422.0\n",
       "3           681.0      25989.0      242683.0\n",
       "4          1939.0      25581.0      113480.0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using dataframe obtained from one of the previous steps\n",
    "df_selKBest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634db662",
   "metadata": {},
   "source": [
    "### Linear Regression model with SelectKBest Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9983a958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.7507\n",
      "RMSE =  4467.404078359277\n"
     ]
    }
   ],
   "source": [
    "# response variable\n",
    "target = df1.enrollment\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df_selKBest,target, test_size=0.25, \n",
    "                                                    random_state=42)\n",
    "# building the model\n",
    "model_kBest = LinearRegression()\n",
    "model_kBest.fit(X_train, Y_train)\n",
    "\n",
    "# predicting the test dataset\n",
    "Y_pred = model_kBest.predict(X_test)\n",
    "\n",
    "# R-squared metric score\n",
    "r2_kBest = model_kBest.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_kBest)\n",
    "\n",
    "# RMSE metric score\n",
    "mse2_kBest = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_kBest**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "18512689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the following information to the lists\n",
    "feature_selection.append(\"SelectKBest\")\n",
    "feature_transformation.append(\"None\")\n",
    "feature_scaling.append(\"None\")\n",
    "r2_values.append(r2_kBest)\n",
    "rmse_values.append(mse2_kBest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f76c884",
   "metadata": {},
   "source": [
    "### 5.j.2) Linear Regression model with SelectKBest Selection and Polynomial Features transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2bdee528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared is  0.7488\n",
      "RMSE is  4484.156850420026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# using Polynomial Features transformation\n",
    "pf_SelectKBest = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "pf_kbest = pf_SelectKBest.fit(df_selKBest)\n",
    "\n",
    "# Using the get_feature_names for the column name\n",
    "poly_SelectKBest= pd.DataFrame(pf_SelectKBest.transform(df_selKBest), columns = pf_kbest.get_feature_names(df_selKBest.columns))\n",
    "poly_SelectKBest\n",
    "\n",
    "# response variable\n",
    "target = df1.enrollment\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(poly_SelectKBest,target, test_size=0.25, random_state=42) \n",
    "      \n",
    "# building the model\n",
    "modelPolynomial_SelectKBest = LinearRegression()\n",
    "modelPolynomial_SelectKBest.fit(X_train, Y_train)    \n",
    "\n",
    "# predicting the test dataset\n",
    "Y_pred = modelPolynomial_SelectKBest.predict(X_test)\n",
    "# R-squared metric score\n",
    "r2_poly_SelectKBest = modelPolynomial_SelectKBest.score(X_test, Y_test).round(4)\n",
    "print(\"R squared is \", r2_poly_SelectKBest)\n",
    "\n",
    "# RMSE metric score\n",
    "mse_poly_SelectKBest = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE is \", mse_poly_SelectKBest**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c04fca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the following information to the lists\n",
    "feature_selection.append(\"SelectKBest\")\n",
    "feature_transformation.append(\"Poly 2\")\n",
    "feature_scaling.append(\"None\")\n",
    "r2_values.append(r2_poly_SelectKBest)\n",
    "rmse_values.append(mse_poly_SelectKBest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947fa6c3",
   "metadata": {},
   "source": [
    "### 5.j.3) Linear Regression model with SelectKBest Selection and Log1P transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8d08e70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.627\n",
      "RMSE =  5464.327063240487\n"
     ]
    }
   ],
   "source": [
    "# log transformation\n",
    "log = lambda x: np.log1p(x)\n",
    "\n",
    "features_log_selKBest = df_selKBest.apply(log)\n",
    "features_log_selKBest\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_log_selKBest,target, test_size=0.25, \n",
    "   random_state=42)\n",
    "# building the linear regression model using SelectKBest Selection\n",
    "model_Log_selKBest = LinearRegression()\n",
    "model_Log_selKBest.fit(X_train, Y_train)\n",
    "\n",
    "# getting the predictions for the test dataset\n",
    "Y_pred = model_Log_selKBest.predict(X_test)\n",
    "\n",
    "# R-squared metric score\n",
    "r2_log_selKBest = model_Log_selKBest.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_log_selKBest)\n",
    "\n",
    "# RMSE metric score\n",
    "mse2_log_selKBest = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_log_selKBest**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "50c25378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the following information to the lists\n",
    "feature_selection.append(\"SelectKBest\")\n",
    "feature_transformation.append(\"Log1P\")\n",
    "feature_scaling.append(\"None\")\n",
    "r2_values.append(r2_log_selKBest)\n",
    "rmse_values.append(mse2_log_selKBest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac9ae49",
   "metadata": {},
   "source": [
    "### 5.j.4) Linear Regression model with SelectKBest Selection and MinMax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8c012123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.7507\n",
      "RMSE =  4467.404078359277\n"
     ]
    }
   ],
   "source": [
    "# minMax scaling\n",
    "msSelectKBest = MinMaxScaler()\n",
    "scaled_features = msSelectKBest.fit_transform(df_selKBest)\n",
    "\n",
    "# Remaking the dataframe\n",
    "features_MinMax_SelectKBest = pd.DataFrame(scaled_features, columns = df_selKBest.columns)\n",
    "features_MinMax_SelectKBest\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_MinMax_SelectKBest, target, test_size=0.25, \n",
    "   random_state=42)\n",
    "\n",
    "# building the linear regression model\n",
    "model_MinMaxSelectKBest = LinearRegression()\n",
    "model_MinMaxSelectKBest.fit(X_train, Y_train)\n",
    "\n",
    "# getting the predictions for the test dataset\n",
    "Y_pred = model_MinMaxSelectKBest.predict(X_test)\n",
    "\n",
    "# R-squared metric score\n",
    "r2_minMaxSelectKBest = model_MinMaxSelectKBest.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_minMaxSelectKBest)\n",
    "\n",
    "# RMSE metric score\n",
    "mse2_minMaxSelectKBest = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_minMaxSelectKBest**0.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "aad53a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the following information to the lists\n",
    "feature_selection.append(\"SelectKBest\")\n",
    "feature_transformation.append(\"None\")\n",
    "feature_scaling.append(\"MinMax\")\n",
    "r2_values.append(r2_minMaxSelectKBest)\n",
    "rmse_values.append(mse2_minMaxSelectKBest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aa21b8",
   "metadata": {},
   "source": [
    "### 5.j.5) Linear Regression model with SelectKBest Selection, Polynomial Features and MinMax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "52df6e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.7488\n",
      "RMSE =  4484.156850518716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#  Polynomial Features transformation\n",
    "pf_SelectKBest = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "pf = pf_SelectKBest.fit(df_selKBest)\n",
    "\n",
    "# Using the get_feature_names for the column name\n",
    "poly_selKBest1= pd.DataFrame(pf_SelectKBest.transform(df_selKBest), columns = pf.get_feature_names(df_selKBest.columns))\n",
    "poly_selKBest1\n",
    "\n",
    "# response variable\n",
    "target = df1.enrollment\n",
    "\n",
    "# minmax scaling\n",
    "ms_polyselKBest = MinMaxScaler()\n",
    "scaled_features = ms_polyselKBest.fit_transform(poly_selKBest1)\n",
    "\n",
    "# Remaking the dataframe\n",
    "features_MinMax_pmin_SelectKBest = pd.DataFrame(scaled_features, columns = poly_selKBest1.columns)\n",
    "features_MinMax_pmin_SelectKBest\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_MinMax_pmin_SelectKBest, target, test_size=0.25, \n",
    "   random_state=42)\n",
    "\n",
    "# building the linear regression model with SelectKBest Selection, Polynomial Features and MinMax Scaling\n",
    "model_MinMax_pmmSelectKBest = LinearRegression()\n",
    "model_MinMax_pmmSelectKBest.fit(X_train, Y_train)\n",
    "\n",
    "# getting the predictions for the test dataset\n",
    "Y_pred = model_MinMax_pmmSelectKBest.predict(X_test)\n",
    "\n",
    "# R-squared metric score\n",
    "r2_minMax_pmmSelectKBest = model_MinMax_pmmSelectKBest.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_minMax_pmmSelectKBest)\n",
    "\n",
    "# RMSE metric score\n",
    "mse2_minMax_pmmSelectKBest = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_minMax_pmmSelectKBest**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8139b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the following information to the lists\n",
    "feature_selection.append(\"SelectKBest\")\n",
    "feature_transformation.append(\"Poly 2\")\n",
    "feature_scaling.append(\"MinMax\")\n",
    "r2_values.append(r2_minMax_pmmSelectKBest)\n",
    "rmse_values.append(mse2_minMax_pmmSelectKBest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015675f",
   "metadata": {},
   "source": [
    "### 5.j.6) Linear Regression model with SelectKBest Selection, Log1P Transformation and MinMax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6d6ed519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.627\n",
      "RMSE =  5464.3270632404865\n"
     ]
    }
   ],
   "source": [
    "log = lambda x: np.log1p(x)\n",
    "# log transformation of selected features\n",
    "features_logMindf_selKBest = df_selKBest.apply(log)\n",
    "features_logMindf_selKBest\n",
    "\n",
    "# minmax scaling\n",
    "ms_logMindf_selKBest = MinMaxScaler()\n",
    "scaled_features = ms_logMindf_selKBest.fit_transform(features_logMindf_selKBest)\n",
    "\n",
    "# Remaking the dataframe\n",
    "features_MinMax_lmdf_selKBest = pd.DataFrame(scaled_features, columns = features_logMindf_selKBest.columns)\n",
    "features_MinMax_lmdf_selKBest\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_MinMax_lmdf_selKBest, target, test_size=0.25, \n",
    "   random_state=42)\n",
    "\n",
    "# building the linear regression model\n",
    "model_MinMax_lmdf_selKBest = LinearRegression()\n",
    "model_MinMax_lmdf_selKBest.fit(X_train, Y_train)\n",
    "\n",
    "# getting the predictions for the test dataset\n",
    "Y_pred = model_MinMax_lmdf_selKBest.predict(X_test)\n",
    "\n",
    "# R-squared metric score\n",
    "r2_minMax_lmdf_selKBest = model_MinMax_lmdf_selKBest.score(X_test, Y_test).round(4)\n",
    "print(\"R2 =\",r2_minMax_lmdf_selKBest)\n",
    "\n",
    "# RMSE metric score\n",
    "mse2_minMax_lmdf_selKBest = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"RMSE = \",mse2_minMax_lmdf_selKBest**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "02488a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the following information to the lists\n",
    "feature_selection.append(\"SelectKBest\")\n",
    "feature_transformation.append(\"Log1P\")\n",
    "feature_scaling.append(\"MinMax\")\n",
    "r2_values.append(r2_minMax_lmdf_selKBest)\n",
    "rmse_values.append(mse2_minMax_lmdf_selKBest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001da331",
   "metadata": {},
   "source": [
    "### 6) Linear Regression Model with Lasso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "226ae0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 9.41182353e-01, 8.82364706e-01, 8.23547059e-01,\n",
       "       7.64729412e-01, 7.05911765e-01, 6.47094118e-01, 5.88276471e-01,\n",
       "       5.29458824e-01, 4.70641176e-01, 4.11823529e-01, 3.53005882e-01,\n",
       "       2.94188235e-01, 2.35370588e-01, 1.76552941e-01, 1.17735294e-01,\n",
       "       5.89176471e-02, 1.00000000e-04])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(features,target, test_size=0.25, \n",
    "                                                    random_state=42)\n",
    "# choosing alpha as 0.0001\n",
    "alphas = np.linspace(1, 0.0001, 18)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "14f882bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 1.000 RMSE 4450.053\n",
      "Alpha 0.941 RMSE 4450.067\n",
      "Alpha 0.882 RMSE 4450.080\n",
      "Alpha 0.824 RMSE 4450.094\n",
      "Alpha 0.765 RMSE 4450.107\n",
      "Alpha 0.706 RMSE 4450.121\n",
      "Alpha 0.647 RMSE 4450.135\n",
      "Alpha 0.588 RMSE 4450.149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.529 RMSE 4450.163\n",
      "Alpha 0.471 RMSE 4450.177\n",
      "Alpha 0.412 RMSE 4450.191\n",
      "Alpha 0.353 RMSE 4450.205\n",
      "Alpha 0.294 RMSE 4450.219\n",
      "Alpha 0.235 RMSE 4450.233\n",
      "Alpha 0.177 RMSE 4450.247\n",
      "Alpha 0.118 RMSE 4450.261\n",
      "Alpha 0.059 RMSE 4450.275\n",
      "Alpha 0.000 RMSE 4450.290\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alpha</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4450.052964</td>\n",
       "      <td>0.778324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.941182</td>\n",
       "      <td>4450.066522</td>\n",
       "      <td>0.778324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.882365</td>\n",
       "      <td>4450.080125</td>\n",
       "      <td>0.778324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.823547</td>\n",
       "      <td>4450.093774</td>\n",
       "      <td>0.778324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.764729</td>\n",
       "      <td>4450.107469</td>\n",
       "      <td>0.778324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.705912</td>\n",
       "      <td>4450.121208</td>\n",
       "      <td>0.778324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.647094</td>\n",
       "      <td>4450.134993</td>\n",
       "      <td>0.778324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.588276</td>\n",
       "      <td>4450.148823</td>\n",
       "      <td>0.778324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.529459</td>\n",
       "      <td>4450.162699</td>\n",
       "      <td>0.778325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.470641</td>\n",
       "      <td>4450.176620</td>\n",
       "      <td>0.778325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.411824</td>\n",
       "      <td>4450.190586</td>\n",
       "      <td>0.778325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.353006</td>\n",
       "      <td>4450.204598</td>\n",
       "      <td>0.778325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.294188</td>\n",
       "      <td>4450.218655</td>\n",
       "      <td>0.778325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.235371</td>\n",
       "      <td>4450.232758</td>\n",
       "      <td>0.778325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.176553</td>\n",
       "      <td>4450.246905</td>\n",
       "      <td>0.778325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.117735</td>\n",
       "      <td>4450.261099</td>\n",
       "      <td>0.778325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.058918</td>\n",
       "      <td>4450.275337</td>\n",
       "      <td>0.778325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>4450.289621</td>\n",
       "      <td>0.778325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Alpha         RMSE        R2\n",
       "0   1.000000  4450.052964  0.778324\n",
       "1   0.941182  4450.066522  0.778324\n",
       "2   0.882365  4450.080125  0.778324\n",
       "3   0.823547  4450.093774  0.778324\n",
       "4   0.764729  4450.107469  0.778324\n",
       "5   0.705912  4450.121208  0.778324\n",
       "6   0.647094  4450.134993  0.778324\n",
       "7   0.588276  4450.148823  0.778324\n",
       "8   0.529459  4450.162699  0.778325\n",
       "9   0.470641  4450.176620  0.778325\n",
       "10  0.411824  4450.190586  0.778325\n",
       "11  0.353006  4450.204598  0.778325\n",
       "12  0.294188  4450.218655  0.778325\n",
       "13  0.235371  4450.232758  0.778325\n",
       "14  0.176553  4450.246905  0.778325\n",
       "15  0.117735  4450.261099  0.778325\n",
       "16  0.058918  4450.275337  0.778325\n",
       "17  0.000100  4450.289621  0.778325"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_list = []\n",
    "r2_list = []\n",
    "model_list = []\n",
    "# for loop\n",
    "for a in alphas:\n",
    "    lasso = Lasso(alpha = a, normalize = False, max_iter = 1000)\n",
    "    lasso.fit(X_train, Y_train)\n",
    "    Y_pred = lasso.predict(X_test)\n",
    "    \n",
    "    r2_list.append(lasso.score(X_train, Y_train))\n",
    "    rmse = mean_squared_error(Y_test, Y_pred)**0.5\n",
    "    rmse_list.append(rmse)## appending the RMSE\n",
    "    \n",
    "    model_list.append(lasso)\n",
    "    print(\"Alpha %.3f RMSE %.3f\" %(a, rmse))\n",
    "\n",
    "# creating the dataframe\n",
    "lasso_df = pd.DataFrame(zip(alphas, rmse_list, r2_list), columns=['Alpha','RMSE','R2'])\n",
    "lasso_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1c810b0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alpha</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4450.052964</td>\n",
       "      <td>0.778324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alpha         RMSE        R2\n",
       "0    1.0  4450.052964  0.778324"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figuring the coefficient of the best result\n",
    "lasso_df.sort_values(by=['RMSE', 'R2'], ascending=[True, False]).head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e4d840de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_rating         1385.581297\n",
      "number_ratings        4.381884\n",
      "last_updated       -194.544658\n",
      "lectures              0.554039\n",
      "duration             14.079809\n",
      "price2              -13.517820\n",
      "discount             21.339283\n",
      "inst_rating         270.501955\n",
      "inst_review          -0.111551\n",
      "inst_student          0.043089\n",
      "inst_course          -7.014475\n",
      "cat_design          306.132419\n",
      "cat_development    -596.585716\n",
      "cat_hobby           521.522464\n",
      "cat_it_software    -558.972094\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# finding the coefficient for the best result using lasso\n",
    "best_model = model_list[0]\n",
    "# display the selected features\n",
    "print(pd.Series(best_model.coef_,index=features.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f12486de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the following information to the lists\n",
    "feature_selection.append(\"Lasso Alpha = 0.0001 Not Normalized\")\n",
    "feature_transformation.append(\"None\")\n",
    "feature_scaling.append(\"None\")\n",
    "r2_values.append(lasso_df.R2[0])\n",
    "rmse_values.append(lasso_df.RMSE[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8f52dd",
   "metadata": {},
   "source": [
    "### 7)  Linear Regression Model with Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "83cc0a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+05, 3.16227766e+04, 1.00000000e+04, 3.16227766e+03,\n",
       "       1.00000000e+03, 3.16227766e+02, 1.00000000e+02, 3.16227766e+01,\n",
       "       1.00000000e+01, 3.16227766e+00, 1.00000000e+00, 3.16227766e-01,\n",
       "       1.00000000e-01, 3.16227766e-02, 1.00000000e-02])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(features,target, test_size=0.25, \n",
    "                                                    random_state=42)\n",
    "# choosing alpha\n",
    "alphas = 10**np.linspace(5, -2, 15)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "88ae1b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 100000.000 RMSE 4476.844\n",
      "Alpha 31622.777 RMSE 4484.460\n",
      "Alpha 10000.000 RMSE 4485.385\n",
      "Alpha 3162.278 RMSE 4480.017\n",
      "Alpha 1000.000 RMSE 4470.570\n",
      "Alpha 316.228 RMSE 4458.812\n",
      "Alpha 100.000 RMSE 4449.104\n",
      "Alpha 31.623 RMSE 4446.888\n",
      "Alpha 10.000 RMSE 4448.324\n",
      "Alpha 3.162 RMSE 4449.500\n",
      "Alpha 1.000 RMSE 4450.015\n",
      "Alpha 0.316 RMSE 4450.200\n",
      "Alpha 0.100 RMSE 4450.261\n",
      "Alpha 0.032 RMSE 4450.281\n",
      "Alpha 0.010 RMSE 4450.287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alpha</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>4476.843711</td>\n",
       "      <td>0.774267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31622.776602</td>\n",
       "      <td>4484.459724</td>\n",
       "      <td>0.774449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>4485.384532</td>\n",
       "      <td>0.774706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3162.277660</td>\n",
       "      <td>4480.017486</td>\n",
       "      <td>0.775189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>4470.569760</td>\n",
       "      <td>0.775878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>316.227766</td>\n",
       "      <td>4458.811584</td>\n",
       "      <td>0.776788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>4449.104292</td>\n",
       "      <td>0.777699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31.622777</td>\n",
       "      <td>4446.887514</td>\n",
       "      <td>0.778166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>4448.323839</td>\n",
       "      <td>0.778295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.162278</td>\n",
       "      <td>4449.500132</td>\n",
       "      <td>0.778320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4450.015301</td>\n",
       "      <td>0.778324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.316228</td>\n",
       "      <td>4450.199936</td>\n",
       "      <td>0.778325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>4450.260961</td>\n",
       "      <td>0.778325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.031623</td>\n",
       "      <td>4450.280542</td>\n",
       "      <td>0.778325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>4450.286763</td>\n",
       "      <td>0.778325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Alpha         RMSE        R2\n",
       "0   100000.000000  4476.843711  0.774267\n",
       "1    31622.776602  4484.459724  0.774449\n",
       "2    10000.000000  4485.384532  0.774706\n",
       "3     3162.277660  4480.017486  0.775189\n",
       "4     1000.000000  4470.569760  0.775878\n",
       "5      316.227766  4458.811584  0.776788\n",
       "6      100.000000  4449.104292  0.777699\n",
       "7       31.622777  4446.887514  0.778166\n",
       "8       10.000000  4448.323839  0.778295\n",
       "9        3.162278  4449.500132  0.778320\n",
       "10       1.000000  4450.015301  0.778324\n",
       "11       0.316228  4450.199936  0.778325\n",
       "12       0.100000  4450.260961  0.778325\n",
       "13       0.031623  4450.280542  0.778325\n",
       "14       0.010000  4450.286763  0.778325"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_list = []\n",
    "r2_list = []\n",
    "model_list = []\n",
    "# for loop\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha = a, normalize = False, max_iter = 1000)\n",
    "    ridge.fit(X_train, Y_train)\n",
    "    Y_pred = ridge.predict(X_test)\n",
    "    \n",
    "    r2_list.append(ridge.score(X_train, Y_train))\n",
    "    rmse1 = mean_squared_error(Y_test, Y_pred)**0.5\n",
    "    rmse_list.append(rmse1) # appending the RMSE\n",
    "    \n",
    "    model_list.append(ridge)\n",
    "    print(\"Alpha %.3f RMSE %.3f\" %(a, rmse1))\n",
    "    \n",
    "ridge_result = np.vstack((alphas, rmse_list, r2_list)).T\n",
    "# creating the dataframe\n",
    "ridge_df = pd.DataFrame(ridge_result, columns=['Alpha','RMSE',\"R2\"])\n",
    "ridge_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e5c63d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alpha</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31.622777</td>\n",
       "      <td>4446.887514</td>\n",
       "      <td>0.778166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Alpha         RMSE        R2\n",
       "7  31.622777  4446.887514  0.778166"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorting the RMSE in ascending order\n",
    "ridge_df.sort_values(by=['RMSE', 'R2'], ascending=[True, False]).head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2a1717e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_rating         795.179309\n",
      "number_ratings       4.389249\n",
      "last_updated      -199.220694\n",
      "lectures             0.587065\n",
      "duration            12.669118\n",
      "price2             -13.594574\n",
      "discount            21.347163\n",
      "inst_rating        438.861178\n",
      "inst_review         -0.110618\n",
      "inst_student         0.042881\n",
      "inst_course         -7.074933\n",
      "cat_design         276.858845\n",
      "cat_development   -502.583366\n",
      "cat_hobby          454.033700\n",
      "cat_it_software   -500.278537\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# finding the coefficient for the best result using Ridge regression\n",
    "best_model = model_list[7]\n",
    "print(pd.Series(best_model.coef_,index=features.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bedaee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the following information to the lists\n",
    "feature_selection.append(\"Ridge Alpha = 0.001 Not Normalized\")\n",
    "feature_transformation.append(\"None\")\n",
    "feature_scaling.append(\"None\")\n",
    "r2_values.append(ridge_df.R2[7])\n",
    "rmse_values.append(ridge_df.RMSE[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12cd2cb",
   "metadata": {},
   "source": [
    "### 8) Plot and summary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "66a27a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Selection</th>\n",
       "      <th>Feature Transformation</th>\n",
       "      <th>Feature Scaling</th>\n",
       "      <th>R2</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.7526</td>\n",
       "      <td>19805077.925240066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All</td>\n",
       "      <td>Poly 2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.5838</td>\n",
       "      <td>33315318.090951506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All</td>\n",
       "      <td>Log1P</td>\n",
       "      <td>None</td>\n",
       "      <td>0.6255</td>\n",
       "      <td>29981300.439621057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.7526</td>\n",
       "      <td>19805077.925240725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All</td>\n",
       "      <td>Poly 2</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.5838</td>\n",
       "      <td>33315333.121404827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>All</td>\n",
       "      <td>Log1P</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.6255</td>\n",
       "      <td>29981300.43962105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Manual</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.7507</td>\n",
       "      <td>19957699.199341107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Manual</td>\n",
       "      <td>Poly 2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.7488</td>\n",
       "      <td>20107662.66057486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Manual</td>\n",
       "      <td>Log1P</td>\n",
       "      <td>None</td>\n",
       "      <td>0.627</td>\n",
       "      <td>29858870.25406242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Manual</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.7507</td>\n",
       "      <td>19957699.199341107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Manual</td>\n",
       "      <td>Poly 2</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.7488</td>\n",
       "      <td>20107662.660053946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Manual</td>\n",
       "      <td>Log1P</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.6255</td>\n",
       "      <td>29981300.43962105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>VT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.7497</td>\n",
       "      <td>20035373.72492288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>VT</td>\n",
       "      <td>Poly 2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.6983</td>\n",
       "      <td>24154950.33365657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>VT</td>\n",
       "      <td>Log1P</td>\n",
       "      <td>None</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>30152303.36838788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>VT</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.7497</td>\n",
       "      <td>20035373.72492166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>VT</td>\n",
       "      <td>Poly 2</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.6983</td>\n",
       "      <td>24154977.376030225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>VT</td>\n",
       "      <td>Log1P</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>30152303.368387878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.7507</td>\n",
       "      <td>19957699.199341107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>Poly 2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.7488</td>\n",
       "      <td>20107662.659168843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>Log1P</td>\n",
       "      <td>None</td>\n",
       "      <td>0.627</td>\n",
       "      <td>29858870.25406241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>None</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.7507</td>\n",
       "      <td>19957699.199341107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>Poly 2</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.7488</td>\n",
       "      <td>20107662.66005393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>Log1P</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.627</td>\n",
       "      <td>29858870.2540624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Lasso Alpha = 0.0001 Not Normalized</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.7783241272792233</td>\n",
       "      <td>4450.052963662444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Ridge Alpha = 0.001 Not Normalized</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.7781663410902708</td>\n",
       "      <td>4446.887514015654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Feature Selection Feature Transformation  \\\n",
       "0                                   All                   None   \n",
       "1                                   All                 Poly 2   \n",
       "2                                   All                  Log1P   \n",
       "3                                   All                   None   \n",
       "4                                   All                 Poly 2   \n",
       "5                                   All                  Log1P   \n",
       "6                                Manual                   None   \n",
       "7                                Manual                 Poly 2   \n",
       "8                                Manual                  Log1P   \n",
       "9                                Manual                   None   \n",
       "10                               Manual                 Poly 2   \n",
       "11                               Manual                  Log1P   \n",
       "12                                   VT                   None   \n",
       "13                                   VT                 Poly 2   \n",
       "14                                   VT                  Log1P   \n",
       "15                                   VT                   None   \n",
       "16                                   VT                 Poly 2   \n",
       "17                                   VT                  Log1P   \n",
       "18                          SelectKBest                   None   \n",
       "19                          SelectKBest                 Poly 2   \n",
       "20                          SelectKBest                  Log1P   \n",
       "21                          SelectKBest                   None   \n",
       "22                          SelectKBest                 Poly 2   \n",
       "23                          SelectKBest                  Log1P   \n",
       "24  Lasso Alpha = 0.0001 Not Normalized                   None   \n",
       "25   Ridge Alpha = 0.001 Not Normalized                   None   \n",
       "\n",
       "   Feature Scaling                  R2                RMSE  \n",
       "0             None              0.7526  19805077.925240066  \n",
       "1             None              0.5838  33315318.090951506  \n",
       "2             None              0.6255  29981300.439621057  \n",
       "3           MinMax              0.7526  19805077.925240725  \n",
       "4           MinMax              0.5838  33315333.121404827  \n",
       "5           MinMax              0.6255   29981300.43962105  \n",
       "6             None              0.7507  19957699.199341107  \n",
       "7             None              0.7488   20107662.66057486  \n",
       "8             None               0.627   29858870.25406242  \n",
       "9           MinMax              0.7507  19957699.199341107  \n",
       "10          MinMax              0.7488  20107662.660053946  \n",
       "11          MinMax              0.6255   29981300.43962105  \n",
       "12            None              0.7497   20035373.72492288  \n",
       "13            None              0.6983   24154950.33365657  \n",
       "14            None              0.6233   30152303.36838788  \n",
       "15          MinMax              0.7497   20035373.72492166  \n",
       "16          MinMax              0.6983  24154977.376030225  \n",
       "17          MinMax              0.6233  30152303.368387878  \n",
       "18            None              0.7507  19957699.199341107  \n",
       "19            None              0.7488  20107662.659168843  \n",
       "20            None               0.627   29858870.25406241  \n",
       "21          MinMax              0.7507  19957699.199341107  \n",
       "22          MinMax              0.7488   20107662.66005393  \n",
       "23          MinMax               0.627    29858870.2540624  \n",
       "24            None  0.7783241272792233   4450.052963662444  \n",
       "25            None  0.7781663410902708   4446.887514015654  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.vstack((feature_selection, feature_transformation, feature_scaling, r2_values, rmse_values)).T\n",
    "data\n",
    "# creating a dataframe\n",
    "df_data = pd.DataFrame(data, columns=['Feature Selection', 'Feature Transformation','Feature Scaling',\n",
    "                                    'R2','RMSE' ])\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a50bb5",
   "metadata": {},
   "source": [
    "From above analysis, I guess, there are some models that have higher r-square value and lower root mean square error. According to our criteria, Lasso and Ridge regression model performed well as they have higer r-square value as compared to other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e5a5fbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_rating         795.179309\n",
      "number_ratings       4.389249\n",
      "last_updated      -199.220694\n",
      "lectures             0.587065\n",
      "duration            12.669118\n",
      "price2             -13.594574\n",
      "discount            21.347163\n",
      "inst_rating        438.861178\n",
      "inst_review         -0.110618\n",
      "inst_student         0.042881\n",
      "inst_course         -7.074933\n",
      "cat_design         276.858845\n",
      "cat_development   -502.583366\n",
      "cat_hobby          454.033700\n",
      "cat_it_software   -500.278537\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sharm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Linear Regression Model to Predict Enrollment in courses')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rmse_list = []\n",
    "r2_list = []\n",
    "model_list = []\n",
    "# for loop\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha = a, normalize = False, max_iter = 1000)\n",
    "    ridge.fit(X_train, Y_train)\n",
    "    Y_pred = ridge.predict(X_test)\n",
    "    \n",
    "    r2_list.append(ridge.score(X_train, Y_train))\n",
    "    rmse1 = mean_squared_error(Y_test, Y_pred)**0.5\n",
    "    rmse_list.append(rmse1) # appending the RMSE\n",
    "    \n",
    "    model_list.append(ridge)\n",
    "#     print(\"Alpha %.3f RMSE %.3f\" %(a, rmse1))\n",
    "    \n",
    "ridge_result = np.vstack((alphas, rmse_list, r2_list)).T\n",
    "# creating the dataframe\n",
    "ridge_df = pd.DataFrame(ridge_result, columns=['Alpha','RMSE',\"R2\"])\n",
    "best_model = model_list[7]\n",
    "print(pd.Series(best_model.coef_,index=features.columns))\n",
    "\n",
    "\n",
    "# plot the prediction vs actual\n",
    "plt.scatter(Y_pred, Y_test, alpha=0.7, color='g')\n",
    "plt.ylabel('Actual price')\n",
    "plt.xlabel('Predicted price')\n",
    "plt.title('Linear Regression Model to Predict Enrollment in courses')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aaeb77",
   "metadata": {},
   "source": [
    "According to me, Ridge regression model performs better as compared to other models. I choose this, as its R square is 77.82% and root mean square error is also 4446.89 which is less as compared to all models. After this, if we want to choose other models then we can choose Lasso regression model as their root mean square error is too low as compared to others. From the actual vs predicted values, we can see there is positive relationship and after 35000 there could be outliers. We can remove those in order to improve our machine learning model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
